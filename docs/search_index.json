[["index.html", "Workshop on Bayesian Data Analysis &amp; Priors Chapter 1 Part I: Introduction 1.1 Structure of this Workshop 1.2 Link to Introductory Video", " Workshop on Bayesian Data Analysis &amp; Priors Riccardo Fusaroli &amp; Chris Cox 2022-01-08 Chapter 1 Part I: Introduction This is a website that provides the lectures and code for our workshop on how to set priors and the Bayesian workflow. 1.1 Structure of this Workshop This workshop takes you through how to set priors in a step-by-step fashion. The course consists of five parts: i) Part 1: General Introduction to Bayesian inference, ii) Part 2: The Gaussian intercepts-only model, iii) Part 3: Linear regression with a dichotomous predictor, iv) Part 4: Multi-level model with repeated measures, v) Part 5: Multi-level model based on informed priors. 1.2 Link to Introductory Video Here is the first video, which provides a soft introduction to the basic concepts of Bayesian inference: https://www.youtube.com/watch?v=_7hlAJ6eWI4&amp;t=213s "],["part-ii-the-gaussian-intercepts-only-model.html", "Chapter 2 Part II: The Gaussian intercepts-only model 2.1 PART I:", " Chapter 2 Part II: The Gaussian intercepts-only model #if you don&#39;t have the pacman package loaded on your computer, uncomment the next line, install pacman, and load in the required packages #install.packages(&#39;pacman&#39;) #load the required packages: pacman::p_load(tidyverse, glue, data.table, dplyr, moments, tidybayes, ggplot2, ggridges, plyr, ellipse, brms, cowplot, viridis) 2.1 PART I: Introduction to the data: Our research question concerns the hyperarticulation hypothesis of infant-directed speech. Do caregivers produce more peripheral vowels in infant-directed speech (IDS) compared to adult-directed speech (ADS)? To answer this question, we manually analysed 9267 extracted vowel tokens from individual caregivers’ IDS and ADS. In order eliminate inter-individual differences in formant values that occur due to physiological characteristics, we first z-score-normalised the formant data according to participant and then computed the total area of each caregiver’s ADS and IDS vowel space. We used this measure to calculate a standardised mean difference score: Cohen’s d. The dataset we will analyse here consists of the following information: i) Subject (i.e. unique participant pseudonym), ii) Register (i.e. the speech style used - adult-directed speech (ADS) vs. infant-directed speech (IDS)), iii) ArticulationS (i.e. the dependent variable - represents a standardised version of the vowel space), iv) ChildSex (i.e. male vs. female), v) ChildAge (i.e. from 11 m to 24 m), vi) First_child (whether this is the caregiver’s first child). #load the data: d &lt;- read.csv(&#39;vowel_space_area_data.csv&#39;) glimpse(d) ## Rows: 48 ## Columns: 7 ## $ X &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1… ## $ Subject &lt;chr&gt; &quot;AF&quot;, &quot;AN&quot;, &quot;CL&quot;, &quot;DM&quot;, &quot;IA&quot;, &quot;JA&quot;, &quot;JE&quot;, &quot;KA&quot;, &quot;KV&quot;, &quot;L… ## $ Register &lt;chr&gt; &quot;ADS&quot;, &quot;ADS&quot;, &quot;ADS&quot;, &quot;ADS&quot;, &quot;ADS&quot;, &quot;ADS&quot;, &quot;ADS&quot;, &quot;ADS&quot;, … ## $ ArticulationS &lt;dbl&gt; 0.76015464, -1.28956792, -0.85914839, -0.81478879, 0.797… ## $ ChildSex &lt;chr&gt; &quot;m&quot;, &quot;m&quot;, &quot;f&quot;, &quot;m&quot;, &quot;f&quot;, &quot;m&quot;, &quot;f&quot;, &quot;m&quot;, &quot;m&quot;, &quot;m&quot;, &quot;m&quot;, &quot;… ## $ ChildAge &lt;int&gt; 20, 22, 16, 12, 23, 24, 21, 14, 16, 16, 12, 16, 23, 12, … ## $ First_child &lt;chr&gt; &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;… Let’s start by building an intercepts-only model with ArticulationS as our dependent variable. We’ll use the function bf() to specify the structure of this first model as follows: Articulation_f1 &lt;- bf(ArticulationS ~ 1) Now that we have our model formula in place, we can use the function get_prior() to tell us which priors need to be specified for this model structure. This function also provides suggestions for default priors: get_prior(Articulation_f1, data = d, family = gaussian) ## prior class coef group resp dpar nlpar bound source ## student_t(3, -0.4, 2.5) Intercept default ## student_t(3, 0, 2.5) sigma default The output of the above function tells us that we need to specify a prior for the intercept (i.e. ‘Intercept’) and the residual variation (i.e. ‘sigma’). This output also provides suggestions for priors (e.g., student_t(3, -0.2, 2.5) for the Intercept), but let’s ignore those for now. Let’s start by specifying weakly informative Gaussian priors for the intercept and sigma, centered at 0 with a standard deviation of 10. These priors allow for a large range of values for the intercept and residual variation (e.g. with this prior, we are specifying that we expect 95 % of the distribution to be between -20 and 20). The tails of these distributions thus represent very large (and unrealistic) values for our parameters of interest - but let’s see where these weakly informative priors take us. weakly_informative_priors &lt;- c( prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = sigma)) Before building an actual model of the hyperarticulation data, we can first run a model to determine whether our prior expectations generate appropriate values. This is known as a prior predictive check. The following code samples from the prior distributions we have specified above, without taking the actual data into account: #I&#39;ve provided comments on the parameters that were not introduced to you in the lecture, #so you have an idea of what they do: Articulation_m1_prior0 &lt;- brm( Articulation_f1, data = d, family = gaussian, prior = weakly_informative_priors, sample_prior = &quot;only&quot;, #this term specifies that the model only samples from the #priors specified. #file = &quot;Articulation_m0_prior0&quot;, #this parameter saves the model in your working #directory to allow you to load the model at a later point. iter = 2000, #this iter parameter specifies the total number of iterations of the #Markov Chain Monte Carlo (MCMC) algorithm. We run prior predictive checks #with a small number of iterations as we are not interested in the statistical #results - we simply use these models to plot samples from our specified priors. warmup = 200, #this parameter specifies number of warmup (aka burn-in) iterations. cores = 2, # Number of CPU cores to use when executing the chains - 2 should be more than # enough, but you can increase this if the models are taking too long to run. chains = 2, # Number of Markov chains (defaults to 4). backend = &quot;cmdstanr&quot;, #feel free to uncomment this and the following line if you have #installed the “cmdstanr” package - runtime will be a little faster. threads = threading(2), control = list(adapt_delta = 0.99, max_treedepth = 20)) #these parameters control the MCMC #sampler’s behaviour in various ways #(e.g., step size). You can leave them like this for now. Let’s plot some predictions from our priors by using the following pp_check() function. This plot shows the predictions from our prior distributions in blue and the actual data in black. This allows us to check whether our specifications of prior distributions are appropriate. pp_check(Articulation_m1_prior0, ndraws = 100) Run the same pp_check() function above a couple of times and observe the results. You’ll notice that the prior predictive plots vary each time you run the code; that’s because the pp_check() function takes random samples from the model we specified above. It’s therefore good practice to run the pp_check() function a couple of times to make sure that the plots show a pattern in the prior predictions of the model. Let’s compare the predictions from our priors (in blue) with those of the data (in black); the predictions appear to have a much larger (and unrealistic) range than that shown by the data. Before moving on, try to answer the following question: Q1: How should we modify our priors to obtain more realistic predictions from our prior distributions? Write your answer here: As mentioned above, by specifying the standard deviations of the above priors as 10, we are saying that we expect approximately 95% of the distribution of standardized measures (i.e. ArticulationS) to be between -20 and 20 sds from the mean; however, given our prior knowledge about the measure, a distance of 20 sds from the mean would be extremely unrealistic. Let’s try to capture our prior knowledge of the measure in our specification of the priors. What would be a reasonable ArticulationS value to expect given our prior knowledge about effect sizes? Effect sizes are usually distributed around ± 2 - to capture this, we can specify a standard deviation of 1 (i.e. with this, we expect approximately 95 % of the distribution to fall within ± 2 x SD ). For the standard deviation of the residual variation, sigma, we expect this to be on the same scale, and as discussed in the slides, we can specify a mean of 1 and standard deviation of 0.5: more_informative_priors &lt;- c( prior(normal(0, 1), class = Intercept), prior(normal(1, 0.5), class = sigma)) Similar to before, then, let’s run a prior predictive check to determine whether these priors provide more appropriate values: Articulation_m1_prior &lt;- brm( Articulation_f1, data = d, family = gaussian, prior = more_informative_priors, sample_prior = &quot;only&quot;, iter = 2000, warmup = 200, backend = &quot;cmdstanr&quot;, threads = threading(2), cores = 2, chains = 2, #file = &quot;Articulation_m1_prior&quot;, control = list(adapt_delta = 0.99, max_treedepth = 20)) Q2: Before you take a look at the prior predictive check plots, how do you expect this model to differ from that of the first and why? Answer: # Prior predictive check for the weakly informative priors: pp_check(Articulation_m1_prior0, ndraws = 100) # Prior predictive check for the more informative priors: pp_check(Articulation_m1_prior, ndraws = 100) Now that our priors appear to be within the order of magnitude of ArticulationS that we expect (i.e. not 10 times more than the range we expect), let’s use these priors to build an intercepts-only model for the actual data: Articulation_m1 &lt;- brm( Articulation_f1, data = d, save_pars = save_pars(all = TRUE), family = gaussian, prior = more_informative_priors, #file = &quot;Articulation_m1&quot;, sample_prior = T, iter = 5000, warmup = 1000, cores = 2, chains = 2, backend = &quot;cmdstanr&quot;, threads = threading(2), control = list( adapt_delta = 0.99, max_treedepth = 15 )) Now that we have our first model, we can conduct posterior predictive checks to make sure that our model has captured the data. pp_check(Articulation_m1, ndraws = 100) The plot indicates that our model captures the overall distribution of our dependent variable (although there is a lot of uncertainty in model predictions). Another way to ensure that our model represents a good fit to the data is to plot prior-posterior update plots. These plots show how our model updates from our priors after seeing the data. To plot these data, we’ll use as_draws_df() to sample from the prior and posterior distributions for the relevant parameters (i.e. ‘Intercept’ and ‘sigma’ in this case) from the above model. #overview of model parameters: variables(Articulation_m1) ## [1] &quot;b_Intercept&quot; &quot;sigma&quot; &quot;Intercept&quot; &quot;prior_Intercept&quot; ## [5] &quot;prior_sigma&quot; &quot;lp__&quot; #Sample the parameters of interest: Posterior_m1 &lt;- as_draws_df(Articulation_m1) #Plot the prior-posterior update plot for the intercept: ggplot(Posterior_m1) + geom_density(aes(prior_Intercept), fill=&quot;steelblue&quot;, color=&quot;black&quot;,alpha=0.6) + geom_density(aes(b_Intercept), fill=&quot;#FC4E07&quot;, color=&quot;black&quot;,alpha=0.6) + theme_classic() #Plot the prior-posterior update plot for sigma: ggplot(Posterior_m1) + geom_density(aes(prior_sigma), fill=&quot;steelblue&quot;, color=&quot;black&quot;,alpha=0.6) + geom_density(aes(sigma), fill=&quot;#FC4E07&quot;, color=&quot;black&quot;,alpha=0.6) + theme_classic() These prior-posterior update plots (with the prior in blue and the posterior in orange) indicate that our model has learned after seeing the data - great stuff! To check your intuitions, try to answer the following question: Q3: How do you think these prior-posterior update plots would compare with those of the weakly informative priors we specified above (repeated below)? weakly_informative_priors &lt;- c( prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = sigma)) Answer: BONUS POINTS: Try to adapt the code for Articulation_m0 and run a model with the weakly-informative priors: Now that we have checked (via posterior predictive checks and prior-posterior update plots) that our model captures relevant information, let’s have a look at the model output: summary(Articulation_m1) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: ArticulationS ~ 1 ## Data: d (Number of observations: 48) ## Draws: 2 chains, each with iter = 4000; warmup = 0; thin = 1; ## total post-warmup draws = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.34 0.15 -0.63 -0.05 1.00 4532 3920 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 1.02 0.11 0.84 1.25 1.00 3630 3742 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The model output gives a summary of the model parameters and then provides estimates of ‘Population-level Effects’ and ‘Family Specific Parameters.’ Let’s go through the ‘Population-level Effects’ and ‘Family Specific Parameters’ from top to bottom and left to right: The Estimate(s) obtained for the Intercept is centered on approx. -0.34 and has an estimated error of 0.15. The 95 % credible interval is approximately [-.63; -.04]). In the ‘Family-Specific Parameters’ section, the residual variation (i.e., sigma) is estimated to be approximately 1.02 [0.85; 1.25]. The Rhat values provide information about the convergence of the algorithm. Rhat values close to 1 suggest that the model has converged. The Bulk_ESS and Tail_ESS (effective sample size (ESS)) capture the sampling efficiency in the bulk and tails of the distribution. "],["part-iii-linear-regression-with-a-dichotomous-predictor.html", "Chapter 3 Part III: Linear regression with a dichotomous predictor", " Chapter 3 Part III: Linear regression with a dichotomous predictor Let’s start off by building a model that includes the constant effect of Register by using the bf() function: Articulation_f2 &lt;- bf(ArticulationS ~ 1 + Register) Let’s see which priors we need to specify for this model by using the same get_prior() function as in Part I: get_prior(Articulation_f2, data = d, family = gaussian) ## prior class coef group resp dpar nlpar bound ## (flat) b ## (flat) b RegisterIDS ## student_t(3, -0.4, 2.5) Intercept ## student_t(3, 0, 2.5) sigma ## source ## default ## (vectorized) ## default ## default The output lets us know that we need to specify priors for the Intercept, slope (i.e. b), and residual variation (i.e. sigma). Let’s use the priors we discussed in the lecture: Articulation_p2 &lt;- c( prior(normal(0, 1), class = Intercept), prior(normal(0, 0.3), class = b), prior(normal(1, 0.5), class = sigma)) Similar to before, let’s perform a prior predictive check to make sure that our prior expectations generate relevant values: Articulation_m2_prior &lt;- brm( Articulation_f2, data = d, save_pars = save_pars(all = TRUE), family = gaussian, prior = Articulation_p2, #file = &quot;Articulation_m2_prior&quot;, sample_prior = &quot;only&quot;, iter = 2000, warmup = 200, cores = 2, chains = 2, backend = &quot;cmdstanr&quot;, threads = threading(2), control = list( adapt_delta = 0.99, max_treedepth = 15 )) Let’s run the pp_check() function a couple of times to check the prior predictions: pp_check(Articulation_m2_prior, ndraws = 100) Great! The samples from our priors appear to be within the order of magnitude that we expect. Let’s run the model with these priors on the actual data: Articulation_m2 &lt;- brm( Articulation_f2, data = d, save_pars = save_pars(all = TRUE), family = gaussian, prior = Articulation_p2, #file = &quot;Articulation_m2&quot;, sample_prior = T, iter = 5000, warmup = 1000, cores = 2, chains = 2, backend = &quot;cmdstanr&quot;, threads = threading(2), control = list( adapt_delta = 0.99, max_treedepth = 15 )) Let’s make sure that the model has captured the data by running the pp_check() function a couple of times: pp_check(Articulation_m2, ndraws = 100) These plots indicate that that the posterior predictions from our model are comparable to the observed data. Perfect! Now that our model includes the constant effect of Register, we can create a plot to explore how Register changes the dependent variable (i.e. ArticulationS). We do this with the conditional_effects() function: # Model inference (population estimate) conditional_effects(Articulation_m2) # Model inference (population estimate) plus actual data plot(conditional_effects(Articulation_m2), points = T) # Model predictions (population estimate + sigma) conditional_effects(Articulation_m2, spaghetti=T, method=&quot;predict&quot;) Q3.5: how do the plots using the “method=”predict\"\" differ from the first two? Answer: Let’s have a look at the prior-posterior update plots for this model: #Sample the parameters of interest: Posterior_m2 &lt;- as_draws_df(Articulation_m2) #Plot the prior-posterior update plot for the intercept: ggplot(Posterior_m2) + geom_density(aes(prior_Intercept), fill=&quot;steelblue&quot;, color=&quot;black&quot;,alpha=0.6) + geom_density(aes(b_Intercept), fill=&quot;#FC4E07&quot;, color=&quot;black&quot;,alpha=0.6) + xlab(&#39;Intercept&#39;) + theme_classic() #Plot the prior-posterior update plot for b: ggplot(Posterior_m2) + geom_density(aes(prior_b), fill=&quot;steelblue&quot;, color=&quot;black&quot;,alpha=0.6) + geom_density(aes(b_RegisterIDS), fill=&quot;#FC4E07&quot;, color=&quot;black&quot;,alpha=0.6) + xlab(&#39;b&#39;) + theme_classic() #Plot the prior-posterior update plot for sigma: ggplot(Posterior_m2) + geom_density(aes(prior_sigma), fill=&quot;steelblue&quot;, color=&quot;black&quot;,alpha=0.6) + geom_density(aes(sigma), fill=&quot;#FC4E07&quot;, color=&quot;black&quot;,alpha=0.6) + xlab(&#39;Sigma&#39;) + theme_classic() Let’s try to read the model output and interpret the findings: summary(Articulation_m2) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: ArticulationS ~ 1 + Register ## Data: d (Number of observations: 48) ## Draws: 2 chains, each with iter = 4000; warmup = 0; thin = 1; ## total post-warmup draws = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.15 0.18 -0.50 0.18 1.00 5984 4791 ## RegisterIDS -0.37 0.21 -0.78 0.04 1.00 6182 4747 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.98 0.10 0.80 1.21 1.00 5818 4892 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Q4: What does this model suggest about the hyperarticulation hypothesis in Danish? Answer: Let’s test our hypothesis that caregivers’ vowel spaces are smaller in IDS by calculating the evidence ratio, as discussed in the lecture, using the hypothesis() function: hypothesis(Articulation_m2, &quot;RegisterIDS &lt; 0&quot;) ## Hypothesis Tests for class b: ## Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob ## 1 (RegisterIDS) &lt; 0 -0.37 0.21 -0.71 -0.03 25.14 0.96 ## Star ## 1 * ## --- ## &#39;CI&#39;: 90%-CI for one-sided and 95%-CI for two-sided hypotheses. ## &#39;*&#39;: For one-sided hypotheses, the posterior probability exceeds 95%; ## for two-sided hypotheses, the value tested against lies outside the 95%-CI. ## Posterior probabilities of point hypotheses assume equal prior probabilities. The evidence ratio of the above constant-effect model appears to provide quite strong evidence in favour of our hypothesis. If you’re a worrier like us, however, you’ll start to second-guess this result and think something like the following: how can we check the extent to which our specification of priors has influenced the estimates? The answer is to conduct a prior robustness check; that is, we can loop through 15 priors with 15 different standard deviations and observe the effect it has on our posterior estimate. We’ll do this in a for loop to make it easy to see what’s going on: # The priors for the above model, repeated here: Articulation_p2 &lt;- c( prior(normal(0, 1), class = Intercept), prior(normal(0, 0.3), class = b), prior(normal(1, 0.5), class = sigma)) # construct a sequence of sds to loop through for the slope prior: priSD &lt;- seq(0.1, 1.5, length.out = 15) priorsN &lt;- Articulation_p2 #create empty variables to store output of the loop: post_pred &lt;- c() post_pred_lci &lt;- c() post_pred_uci &lt;- c() for (i in 1:length(priSD)) { priorsN[2,] &lt;- set_prior(paste0(&quot;normal(0, &quot;, priSD[i],&quot;)&quot;), class = &quot;b&quot;) model_for_loop &lt;- brm(Articulation_f2, data = d, family = gaussian, prior = priorsN, sample_prior = T, warmup = 1000, iter = 5000, cores = 2, chains = 2, backend = &quot;cmdstanr&quot;, threads = threading(2), save_pars = save_pars(all = TRUE), control = list(adapt_delta = 0.99, max_treedepth = 15)) post_preds &lt;- spread_draws(model_for_loop, b_RegisterIDS) post_pred[i] &lt;- median(post_preds$b_RegisterIDS) post_pred_lci[i] &lt;- quantile(post_preds$b_RegisterIDS, prob = 0.025) post_pred_uci[i] &lt;- quantile(post_preds$b_RegisterIDS, prob = 0.975) } models_data &lt;- tibble(priSD, post_pred, post_pred_lci, post_pred_uci) ggplot(data=models_data, aes(x=priSD, y=post_pred)) + geom_point(size = 3) + geom_pointrange(ymin = post_pred_lci, ymax = post_pred_uci) + ylim(-1.3, 0.3) + labs(x=&quot;Standard Deviation of Slope Prior&quot;, y=&quot;Posterior Estimate for slope&quot;, title=&quot;Sensitivity analysis for constant-effect model&quot;) + theme_bw() + theme(plot.title = element_text(hjust = 0.5, size = 15), axis.title.x = element_text(size = 13), axis.text.y = element_text(size = 12), axis.text.x = element_text(size = 12), axis.title.y = element_text(size = 13)) Code to model heteroskedasticity (unequal variance): Now that we’ve calmed our worries about the influence of our priors, we can feel slightly more confident in our results. But what if we’re also worried about the influence of heteroskedasticity (beyond how to pronounce it)? Don’t worry - we can simply model the variance in the two groups: bonus_model &lt;- bf(ArticulationS ~ 1 + Register, sigma ~ 1 + Register) get_prior(bonus_model, data = d, family = gaussian) bonus_priors &lt;- c( prior(normal(0, 1), class = Intercept), prior(normal(0, 0.3), class = b), prior(normal(1, 0.5), class = Intercept, dpar=sigma), prior(normal(0, 1), class = b, dpar=sigma) ) bonus_model_prior &lt;- brm( bonus_model, data = d, save_pars = save_pars(all = TRUE), family = gaussian, prior = bonus_priors, #file = &quot;bonus_model_prior&quot;, sample_prior = &quot;only&quot;, iter = 2000, cores = 2, chains = 2, backend = &quot;cmdstanr&quot;, threads = threading(2), control = list( adapt_delta = 0.99, max_treedepth = 15 )) pp_check(bonus_model_prior, ndraws=50) bonus_model_m &lt;- brm( bonus_model, data = d, save_pars = save_pars(all = TRUE), family = gaussian, prior = bonus_priors, #file = &quot;bonus_model_m&quot;, sample_prior = T, iter = 2000, cores = 2, chains = 2, backend = &quot;cmdstanr&quot;, threads = threading(2), control = list( adapt_delta = 0.99, max_treedepth = 15 )) pp_check(bonus_model_m, ndraws=50) plot(conditional_effects(bonus_model_m), points = T) #let&#39;s compare this to the other model: plot(conditional_effects(Articulation_m2), points = T) summary(bonus_model_m) ## Family: gaussian ## Links: mu = identity; sigma = log ## Formula: ArticulationS ~ 1 + Register ## sigma ~ 1 + Register ## Data: d (Number of observations: 48) ## Draws: 2 chains, each with iter = 1000; warmup = 0; thin = 1; ## total post-warmup draws = 2000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.15 0.18 -0.51 0.19 1.00 1575 1358 ## sigma_Intercept -0.03 0.16 -0.32 0.30 1.00 1403 1157 ## RegisterIDS -0.35 0.21 -0.78 0.08 1.00 1563 947 ## sigma_RegisterIDS 0.10 0.22 -0.33 0.53 1.00 1420 1001 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). "],["part-iv-multi-level-model-with-repeated-measures.html", "Chapter 4 Part IV: Multi-level model with repeated measures 4.1 PART IV:", " Chapter 4 Part IV: Multi-level model with repeated measures 4.1 PART IV: Let’s take our analysis to the next level (pun intended) by making a multi-level model with varying intercepts and varying slopes. In this model, we assign unique intercepts and slopes for each subject and assign them a common prior distribution. Let’s check which priors we need to specify for this model: Articulation_f3 &lt;- bf(ArticulationS ~ 1 + Register + (1+Register|Subject)) get_prior(Articulation_f3, data = d, family = gaussian) ## prior class coef group resp dpar nlpar bound ## (flat) b ## (flat) b RegisterIDS ## lkj(1) cor ## lkj(1) cor Subject ## student_t(3, -0.4, 2.5) Intercept ## student_t(3, 0, 2.5) sd ## student_t(3, 0, 2.5) sd Subject ## student_t(3, 0, 2.5) sd Intercept Subject ## student_t(3, 0, 2.5) sd RegisterIDS Subject ## student_t(3, 0, 2.5) sigma ## source ## default ## (vectorized) ## default ## (vectorized) ## default ## default ## (vectorized) ## (vectorized) ## (vectorized) ## default The output of the above function tells us that we now have three sources of variation in the model: i) the usual standard deviation of the residuals (i.e., ‘sigma’), ii) the standard deviation of the population of by-subject varying intercepts (i.e., ‘Intercept’), and ii) the standard deviation of the population of by-subject varying slopes (i.e., ‘RegisterIDS’). The latter two sources of variation provide one of the most essential features of multi-level models: partial pooling, as we discussed in the lecture. We are also told that we need to specify a prior for the correlation between the varying intercepts and the varying slopes (i.e. class = ‘cor’). This correlation captures the fact that we cannot assume the intercept and slope to be entirely independent. For example, caregivers who naturally produce a small vowel space in ADS can expand their vowel space when using IDS to a greater extent and may therefore show stronger effects. We model the correlation between varying intercepts and slopes by using a so-called LKJ prior. The basic idea of the LKJ prior is that as its parameter increases, the prior favors less extreme correlations - that is, if we specify the LKJ parameter as 1, the prior represents a uniform, uninformative distribution (similar to that of a beta(1,1) distribution). If we set the parameter to 2, on the other hand, the prior serves to dampen the likelihood of extreme correlations, and we represent our lack of knowledge about the extent of correlation between the parameters. We’ll see what our prior(lkj(2), class = cor) looks like in the prior-posterior update plot below. As usual, let’s start by performing a prior predictive check: Articulation_p3 &lt;- c( prior(normal(0, 1), class = Intercept), prior(normal(0, 1), class = sd, coef = Intercept, group = Subject), prior(normal(0, 0.3), class = b), prior(normal(0, 1), class = sd, coef = RegisterIDS, group = Subject), prior(normal(1, 0.5), class = sigma), prior(lkj(2), class = cor)) Articulation_m3_prior &lt;- brm( Articulation_f3, data = d, save_pars = save_pars(all = TRUE), family = gaussian, prior = Articulation_p3, #file = &quot;Articulation_m3_prior&quot;, sample_prior = &quot;only&quot;, iter = 5000, warmup = 1000, cores = 2, chains = 2, backend = &quot;cmdstanr&quot;, threads = threading(2), control = list( adapt_delta = 0.999, max_treedepth = 20)) pp_check(Articulation_m3_prior, ndraws=100) After running the pp_check() function a couple of times, we should be convinced that our priors are within the order of magnitude that we expect. Great, let’s run the model on the actual data then: Articulation_m3 &lt;- brm( Articulation_f3, data = d, save_pars = save_pars(all = TRUE), family = gaussian, prior = Articulation_p3, #file = &quot;Articulation_m3&quot;, sample_prior = T, iter = 5000, warmup = 1000, cores = 2, chains = 2, backend = &quot;cmdstanr&quot;, threads = threading(2), control = list( adapt_delta = 0.999, max_treedepth = 20)) As usual, let’s perform some posterior predictive checks to make sure that the model captures the data: pp_check(Articulation_m3, ndraws=100) Everything looks great, so let’s plot the conditional effects and compare the output of the multi-level model with that of the previous constant-effect model: plot(conditional_effects(Articulation_m3), points = T) #summary of the multi-level model: summary(Articulation_m3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: ArticulationS ~ 1 + Register + (1 + Register | Subject) ## Data: d (Number of observations: 48) ## Draws: 2 chains, each with iter = 4000; warmup = 0; thin = 1; ## total post-warmup draws = 8000 ## ## Group-Level Effects: ## ~Subject (Number of levels: 24) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS ## sd(Intercept) 0.42 0.24 0.03 0.91 1.00 981 ## sd(RegisterIDS) 0.47 0.31 0.02 1.12 1.00 892 ## cor(Intercept,RegisterIDS) -0.04 0.43 -0.79 0.79 1.00 3220 ## Tail_ESS ## sd(Intercept) 1001 ## sd(RegisterIDS) 733 ## cor(Intercept,RegisterIDS) 5085 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.13 0.19 -0.50 0.24 1.00 6569 5557 ## RegisterIDS -0.39 0.21 -0.78 0.03 1.00 9691 6025 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.83 0.15 0.50 1.12 1.00 713 388 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). #summary of the constant-effect model: summary(Articulation_m2) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: ArticulationS ~ 1 + Register ## Data: d (Number of observations: 48) ## Draws: 2 chains, each with iter = 4000; warmup = 0; thin = 1; ## total post-warmup draws = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.15 0.18 -0.50 0.18 1.00 5984 4791 ## RegisterIDS -0.37 0.21 -0.78 0.04 1.00 6182 4747 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.98 0.10 0.80 1.21 1.00 5818 4892 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Q5: What happens to the estimate for sigma (i.e. the residual variation) in this multi-level model? What does this indicate? Answer: Let’s create some prior-posterior update plots so we can visualise how our model updates after seeing the data: #Sample the parameters of interest: Posterior_m3 &lt;- as_draws_df(Articulation_m3) #Plot the prior-posterior update plot for the intercept: ggplot(Posterior_m3) + geom_density(aes(prior_Intercept), fill=&quot;steelblue&quot;, color=&quot;black&quot;,alpha=0.6) + geom_density(aes(b_Intercept), fill=&quot;#FC4E07&quot;, color=&quot;black&quot;,alpha=0.6) + xlab(&#39;Intercept&#39;) + theme_classic() #Plot the prior-posterior update plot for b: ggplot(Posterior_m3) + geom_density(aes(prior_b), fill=&quot;steelblue&quot;, color=&quot;black&quot;,alpha=0.6) + geom_density(aes(b_RegisterIDS), fill=&quot;#FC4E07&quot;, color=&quot;black&quot;,alpha=0.6) + xlab(&#39;b&#39;) + theme_classic() #Plot the prior-posterior update plot for sd of intercepts and slopes: ggplot(Posterior_m3) + geom_density(aes(sd_Subject__Intercept), fill=&quot;#FC4E07&quot;, color=&quot;black&quot;,alpha=0.3) + geom_density(aes(sd_Subject__RegisterIDS), fill=&quot;#228B22&quot;, color=&quot;black&quot;,alpha=0.4) + geom_density(aes(prior_sd_Subject__RegisterIDS), fill=&quot;steelblue&quot;, color=&quot;black&quot;,alpha=0.6) + xlab(&#39;sd&#39;) + theme_classic() #Plot the prior-posterior update plot for sigma: ggplot(Posterior_m3) + geom_density(aes(prior_sigma), fill=&quot;steelblue&quot;, color=&quot;black&quot;,alpha=0.6) + geom_density(aes(sigma), fill=&quot;#FC4E07&quot;, color=&quot;black&quot;,alpha=0.6) + xlab(&#39;sigma&#39;) + theme_classic() #Plot the prior-posterior update plot for the correlation between varying intercepts and slopes: ggplot(Posterior_m3) + geom_density(aes(prior_cor_Subject), fill=&quot;steelblue&quot;, color=&quot;black&quot;,alpha=0.6) + geom_density(aes(cor_Subject__Intercept__RegisterIDS), fill=&quot;#FC4E07&quot;, color=&quot;black&quot;,alpha=0.6) + xlab(&#39;cor&#39;) + theme_classic() Let’s test our hypothesis with this model by using the hypothesis() function: #overall hypothesis(Articulation_m3, &quot;RegisterIDS &lt; 0&quot;) ## Hypothesis Tests for class b: ## Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob ## 1 (RegisterIDS) &lt; 0 -0.39 0.21 -0.72 -0.05 29.65 0.97 ## Star ## 1 * ## --- ## &#39;CI&#39;: 90%-CI for one-sided and 95%-CI for two-sided hypotheses. ## &#39;*&#39;: For one-sided hypotheses, the posterior probability exceeds 95%; ## for two-sided hypotheses, the value tested against lies outside the 95%-CI. ## Posterior probabilities of point hypotheses assume equal prior probabilities. #for individual subjectss: hypothesis(Articulation_m3, &quot;RegisterIDS &lt; 0&quot;, group = &quot;Subject&quot;, scope=&quot;coef&quot;) ## Hypothesis Tests for class : ## Group Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio ## 1 AF (RegisterIDS) &lt; 0 0.11 0.64 -0.70 1.36 1.07 ## 2 AN (RegisterIDS) &lt; 0 -0.67 0.53 -1.65 0.07 13.84 ## 3 CL (RegisterIDS) &lt; 0 -0.31 0.46 -1.01 0.52 3.62 ## 4 DM (RegisterIDS) &lt; 0 -0.35 0.46 -1.08 0.44 4.13 ## 5 IA (RegisterIDS) &lt; 0 -0.42 0.46 -1.20 0.34 5.38 ## 6 JA (RegisterIDS) &lt; 0 -0.89 0.68 -2.28 -0.07 29.77 ## 7 JE (RegisterIDS) &lt; 0 -0.43 0.45 -1.17 0.31 5.70 ## 8 KA (RegisterIDS) &lt; 0 -0.52 0.47 -1.35 0.18 8.27 ## 9 KV (RegisterIDS) &lt; 0 -0.54 0.48 -1.38 0.19 8.85 ## 10 LK (RegisterIDS) &lt; 0 -0.66 0.54 -1.69 0.08 12.77 ## 11 LM (RegisterIDS) &lt; 0 -0.28 0.46 -0.98 0.52 3.38 ## 12 LV (RegisterIDS) &lt; 0 -0.29 0.47 -0.98 0.55 3.63 ## 13 MA (RegisterIDS) &lt; 0 -0.38 0.51 -1.25 0.45 4.22 ## 14 ME (RegisterIDS) &lt; 0 -0.53 0.47 -1.36 0.18 8.93 ## 15 MPS (RegisterIDS) &lt; 0 -0.48 0.45 -1.24 0.23 7.82 ## 16 MS (RegisterIDS) &lt; 0 -0.48 0.45 -1.26 0.23 7.21 ## 17 MV (RegisterIDS) &lt; 0 -0.14 0.51 -0.83 0.84 2.07 ## 18 PV (RegisterIDS) &lt; 0 -0.30 0.51 -1.10 0.57 3.26 ## 19 RV (RegisterIDS) &lt; 0 -0.25 0.48 -0.97 0.63 2.92 ## 20 SA (RegisterIDS) &lt; 0 -0.72 0.55 -1.77 0.02 16.54 ## 21 SAF (RegisterIDS) &lt; 0 -0.40 0.46 -1.17 0.37 5.13 ## 22 SC (RegisterIDS) &lt; 0 -0.36 0.45 -1.06 0.41 4.52 ## 23 TL (RegisterIDS) &lt; 0 -0.78 0.59 -1.93 -0.04 23.17 ## 24 TM (RegisterIDS) &lt; 0 -0.57 0.48 -1.46 0.12 11.10 ## Post.Prob Star ## 1 0.52 ## 2 0.93 ## 3 0.78 ## 4 0.81 ## 5 0.84 ## 6 0.97 * ## 7 0.85 ## 8 0.89 ## 9 0.90 ## 10 0.93 ## 11 0.77 ## 12 0.78 ## 13 0.81 ## 14 0.90 ## 15 0.89 ## 16 0.88 ## 17 0.67 ## 18 0.77 ## 19 0.75 ## 20 0.94 ## 21 0.84 ## 22 0.82 ## 23 0.96 * ## 24 0.92 ## --- ## &#39;CI&#39;: 90%-CI for one-sided and 95%-CI for two-sided hypotheses. ## &#39;*&#39;: For one-sided hypotheses, the posterior probability exceeds 95%; ## for two-sided hypotheses, the value tested against lies outside the 95%-CI. ## Posterior probabilities of point hypotheses assume equal prior probabilities. These results look interesting - however, we may be worried about the influence of our priors. Let’s conduct a prior robustness check for this multi-level model to calm our worries: #code to loop through sd of intercept prior: priSD &lt;- seq(0.1, 1.5, length.out = 15) priorsN &lt;- Articulation_p3 #create empty sets to store output of the loop: post_pred &lt;- c() post_pred_lci &lt;- c() post_pred_uci &lt;- c() for (i in 1:length(priSD)) { priorsN[3,] &lt;- set_prior(paste0(&quot;normal(0, &quot;, priSD[i],&quot;)&quot;), class = &quot;b&quot;) model_for_loop &lt;- brm(Articulation_f3, data = d, family = gaussian, prior = priorsN, sample_prior = T, warmup = 1000, iter = 5000, cores = 2, chains = 2, backend = &quot;cmdstanr&quot;, threads = threading(2), save_pars = save_pars(all = TRUE), control = list(adapt_delta = 0.99, max_treedepth = 15)) post_preds &lt;- spread_draws(model_for_loop, b_RegisterIDS) post_pred[i] &lt;- median(post_preds$b_RegisterIDS) post_pred_lci[i] &lt;- quantile(post_preds$b_RegisterIDS, prob = 0.025) post_pred_uci[i] &lt;- quantile(post_preds$b_RegisterIDS, prob = 0.975) } models_data &lt;- data.frame(priSD, post_pred, post_pred_lci, post_pred_uci) ggplot(data=models_data, aes(x=priSD, y=post_pred)) + geom_point(size = 3) + geom_pointrange(ymin = post_pred_lci, ymax = post_pred_uci) + ylim(-1.3, 0.3) + labs(x=&quot;Standard Deviation of Slope Prior&quot;, y=&quot;Posterior Estimate for slope&quot;, title=&quot;Sensitivity analysis for multi-level model&quot;) + theme_bw() + theme(plot.title = element_text(hjust = 0.5, size = 15), axis.title.x = element_text(size = 13), axis.text.y = element_text(size = 12), axis.text.x = element_text(size = 12), axis.title.y = element_text(size = 13)) Q6: How does this prior robustness check compare to that of the model in Part 2? Answer: BONUS CONTENT on how to plot partial pooling: plot_df &lt;- tibble( Subject = rownames(coef(Articulation_m3)[[&quot;Subject&quot;]][,,&quot;Intercept&quot;]), ADS = coef(Articulation_m3)[[&quot;Subject&quot;]][,,&quot;Intercept&quot;][,1], IDS = ADS + coef(Articulation_m3)[[&quot;Subject&quot;]][,,&quot;RegisterIDS&quot;][,1], Type = &quot;partial pooling&quot; ) %&gt;% pivot_longer(ADS:IDS) %&gt;% dplyr::rename( Register = name, ArticulationS = value ) df &lt;- d[, c(&quot;Subject&quot;, &quot;Register&quot;, &quot;ArticulationS&quot;)] %&gt;% mutate(Type = &quot;no pooling&quot;) pool_df &lt;- df[,c(&quot;Subject&quot;, &quot;Register&quot;)] %&gt;% mutate( ArticulationS = ifelse(Register==&quot;ADS&quot;, mean(df$ArticulationS[df$Register==&quot;ADS&quot;]), mean(df$ArticulationS[df$Register==&quot;IDS&quot;])), Type = &quot;total pooling&quot; ) plot_df &lt;- rbind(plot_df,df) plot_df &lt;- rbind(plot_df,pool_df) plot_df &lt;- plot_df %&gt;% mutate(Register=as.numeric(as.factor(Register))) ggplot(plot_df, aes(Register, ArticulationS, color = Type)) + geom_path(size = 1) + geom_point() + facet_wrap(.~Subject) + scale_x_continuous(breaks=seq(1, 2, 1)) + theme_bw() + theme(axis.title.x = element_text(size = 13), axis.text.y = element_text(size = 12), axis.text.x = element_text(size = 12), axis.title.y = element_text(size = 13), strip.background = element_rect(color=&quot;white&quot;, fill=&quot;white&quot;, size=1.5, linetype=&quot;solid&quot;), strip.text.x = element_text(size = 10, color = &quot;black&quot;)) ### Now the ellipsis plot ## Partial pooling df_partial &lt;- tibble( Subject = rownames(coef(Articulation_m3)[[&quot;Subject&quot;]][,,&quot;Intercept&quot;]), ADS = coef(Articulation_m3)[[&quot;Subject&quot;]][,,&quot;Intercept&quot;][,1], RegisterIDS = coef(Articulation_m3)[[&quot;Subject&quot;]][,,&quot;RegisterIDS&quot;][,1], Type = &quot;Partial pooling&quot; ) ## Original data df_no &lt;- NULL for (s in unique(d$Subject)){ tmp &lt;- tibble( Subject = s, ADS = d$ArticulationS[d$Register==&quot;ADS&quot; &amp; d$Subject==s], RegisterIDS = d$ArticulationS[d$Register==&quot;IDS&quot; &amp; d$Subject==s] - d$ArticulationS[d$Register==&quot;ADS&quot; &amp; d$Subject==s], Type = &quot;No pooling&quot; ) if (exists(&quot;df_no&quot;)){df_no = rbind(df_no, tmp)} else {df_no = tmp} } df_total &lt;- df_no[,c(&quot;Subject&quot;)] %&gt;% mutate( ADS = mean(d$ArticulationS[d$Register==&quot;ADS&quot;]), RegisterIDS = mean(d$ArticulationS[d$Register==&quot;IDS&quot;]) - mean(d$ArticulationS[d$Register==&quot;ADS&quot;]), Type = &quot;Total pooling&quot; ) df_fixef &lt;- tibble( Type = &quot;Partial pooling (average)&quot;, ADS = fixef(Articulation_m3)[1], RegisterIDS = fixef(Articulation_m3)[2] ) # Complete pooling / fixed effects are center of gravity in the plot df_gravity &lt;- df_total %&gt;% distinct(Type, ADS, RegisterIDS) %&gt;% bind_rows(df_fixef) df_gravity ## # A tibble: 2 × 3 ## ADS RegisterIDS Type ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1.83e-16 -0.691 Total pooling ## 2 -1.26e- 1 -0.387 Partial pooling (average) df_pulled &lt;- bind_rows(df_no, df_partial) # Extract the variance covariance matrix cov_mat_t &lt;- VarCorr(Articulation_m3)[[&quot;Subject&quot;]]$cov cov_mat &lt;- matrix(nrow=2, ncol=2) cov_mat[1,1]&lt;-cov_mat_t[,,&quot;Intercept&quot;][1,1] cov_mat[2,1]&lt;-cov_mat_t[,,&quot;RegisterIDS&quot;][1,1] cov_mat[1,2]&lt;-cov_mat_t[,,&quot;Intercept&quot;][2,1] cov_mat[2,2]&lt;-cov_mat_t[,,&quot;RegisterIDS&quot;][2,1] cov_mat ## [,1] [,2] ## [1,] 0.23098684 -0.03787621 ## [2,] -0.03787621 0.30976748 make_ellipse &lt;- function(cov_mat, center, level) { ellipse(cov_mat, centre = center, level = level) %&gt;% as.data.frame() %&gt;% add_column(level = level) %&gt;% as_tibble() } center &lt;- fixef(Articulation_m3) levels &lt;- c(.1, .3, .5, .7, .9) # Create an ellipse dataframe for each of the levels defined # above and combine them df_ellipse &lt;- levels %&gt;% purrr::map_df(~ make_ellipse(cov_mat, center, level = .x)) %&gt;% dplyr::rename(ADS = x, RegisterIDS = y) df_ellipse ## # A tibble: 500 × 3 ## ADS RegisterIDS level ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0187 -0.220 0.1 ## 2 0.00783 -0.208 0.1 ## 3 -0.00357 -0.197 0.1 ## 4 -0.0155 -0.186 0.1 ## 5 -0.0278 -0.177 0.1 ## 6 -0.0405 -0.168 0.1 ## 7 -0.0536 -0.160 0.1 ## 8 -0.0670 -0.153 0.1 ## 9 -0.0806 -0.147 0.1 ## 10 -0.0944 -0.142 0.1 ## # … with 490 more rows Gaussian_ellipsis &lt;- ggplot(df_pulled) + aes(x = ADS, y = RegisterIDS, color = Type) + # Draw contour lines from the distribution of effects geom_path( aes(group = level, color = NULL), data = df_ellipse, linetype = &quot;dashed&quot;, color = &quot;grey40&quot; ) + geom_point(data = df_gravity, size = 5) + geom_point(size = 2) + geom_path( aes(group = Subject, color = NULL), arrow = arrow(length = unit(.02, &quot;npc&quot;)) ) + # Use ggrepel to jitter the labels away from the points ggrepel::geom_text_repel( aes(label = Subject, color = NULL), data = df_no ) + # Don&#39;t forget 373 ggrepel::geom_text_repel( aes(label = Subject, color = NULL), data = df_partial ) + ggtitle(&quot;Topographic map of regression parameters&quot;) + xlab(&quot;Intercept estimate&quot;) + ylab(&quot;Slope estimate&quot;) + scale_color_brewer(palette = &quot;Dark2&quot;) + theme_classic() + theme(plot.title = element_text(hjust = 0.5, size = 15), legend.position = &quot;bottom&quot;, axis.title.x = element_text(size = 13), axis.text.y = element_text(size = 12), axis.text.x = element_text(size = 12), axis.title.y = element_text(size = 13), strip.background = element_rect(color=&quot;white&quot;, fill=&quot;white&quot;, size=1.5, linetype=&quot;solid&quot;)) Gaussian_ellipsis ## Warning: ggrepel: 13 unlabeled data points (too many overlaps). Consider ## increasing max.overlaps BONUS CONTENT on Student t: One of the ways to allow for more flexibility in the distribution of the data would be to model the data using a student t likelihood. The main difference between the Gaussian and student t distribution is that a student t distribution with a low degrees of freedom parameter, nu, has heavier tails than the conventional Gaussian distribution. This form of model thus dampens the influence of outliers - incorporating outliers without allowing them to dominate non-outlier data. You can specify a student likelihood in the model using the ‘family’ parameter. As we can see in the results of the below get_prior() function, we now have to provide an additional prior for nu. For the following student t model, we will set the prior to be gamma(2, 0.1): Articulation_f3 &lt;- bf(ArticulationS ~ 1 + Register + (1+Register|Subject)) get_prior(Articulation_f3, data = d, family = student) ## prior class coef group resp dpar nlpar bound ## (flat) b ## (flat) b RegisterIDS ## lkj(1) cor ## lkj(1) cor Subject ## student_t(3, -0.4, 2.5) Intercept ## gamma(2, 0.1) nu ## student_t(3, 0, 2.5) sd ## student_t(3, 0, 2.5) sd Subject ## student_t(3, 0, 2.5) sd Intercept Subject ## student_t(3, 0, 2.5) sd RegisterIDS Subject ## student_t(3, 0, 2.5) sigma ## source ## default ## (vectorized) ## default ## (vectorized) ## default ## default ## default ## (vectorized) ## (vectorized) ## (vectorized) ## default student_priors &lt;- c( prior(normal(0, 1), class = Intercept), prior(normal(0, 1), class = sd, coef = Intercept, group = Subject), prior(normal(0, 0.3), class = b), prior(normal(0, 1), class = sd, coef = RegisterIDS, group = Subject), prior(normal(1, 0.5), class = sigma), prior(lkj(2), class = cor), prior(gamma(2, 0.1), class = nu)) Articulation_student_m3 &lt;- brm( Articulation_f3, data = d, save_pars = save_pars(all = TRUE), family = student, prior = student_priors, #file = &quot;Articulation_student_m3&quot;, sample_prior = T, iter = 10000, warmup = 1000, cores = 2, chains = 2, backend = &quot;cmdstanr&quot;, threads = threading(2), control = list( adapt_delta = 0.999, max_treedepth = 20)) ## Compiling Stan program... ## Start sampling ## Running MCMC with 2 parallel chains, with 2 thread(s) per chain... ## ## Chain 1 Iteration: 1 / 10000 [ 0%] (Warmup) ## Chain 2 Iteration: 1 / 10000 [ 0%] (Warmup) ## Chain 2 Iteration: 100 / 10000 [ 1%] (Warmup) ## Chain 1 Iteration: 100 / 10000 [ 1%] (Warmup) ## Chain 2 Iteration: 200 / 10000 [ 2%] (Warmup) ## Chain 1 Iteration: 200 / 10000 [ 2%] (Warmup) ## Chain 2 Iteration: 300 / 10000 [ 3%] (Warmup) ## Chain 1 Iteration: 300 / 10000 [ 3%] (Warmup) ## Chain 2 Iteration: 400 / 10000 [ 4%] (Warmup) ## Chain 1 Iteration: 400 / 10000 [ 4%] (Warmup) ## Chain 2 Iteration: 500 / 10000 [ 5%] (Warmup) ## Chain 1 Iteration: 500 / 10000 [ 5%] (Warmup) ## Chain 1 Iteration: 600 / 10000 [ 6%] (Warmup) ## Chain 2 Iteration: 600 / 10000 [ 6%] (Warmup) ## Chain 1 Iteration: 700 / 10000 [ 7%] (Warmup) ## Chain 2 Iteration: 700 / 10000 [ 7%] (Warmup) ## Chain 1 Iteration: 800 / 10000 [ 8%] (Warmup) ## Chain 2 Iteration: 800 / 10000 [ 8%] (Warmup) ## Chain 1 Iteration: 900 / 10000 [ 9%] (Warmup) ## Chain 2 Iteration: 900 / 10000 [ 9%] (Warmup) ## Chain 1 Iteration: 1000 / 10000 [ 10%] (Warmup) ## Chain 1 Iteration: 1001 / 10000 [ 10%] (Sampling) ## Chain 2 Iteration: 1000 / 10000 [ 10%] (Warmup) ## Chain 2 Iteration: 1001 / 10000 [ 10%] (Sampling) ## Chain 1 Iteration: 1100 / 10000 [ 11%] (Sampling) ## Chain 1 Iteration: 1200 / 10000 [ 12%] (Sampling) ## Chain 2 Iteration: 1100 / 10000 [ 11%] (Sampling) ## Chain 1 Iteration: 1300 / 10000 [ 13%] (Sampling) ## Chain 1 Iteration: 1400 / 10000 [ 14%] (Sampling) ## Chain 2 Iteration: 1200 / 10000 [ 12%] (Sampling) ## Chain 1 Iteration: 1500 / 10000 [ 15%] (Sampling) ## Chain 1 Iteration: 1600 / 10000 [ 16%] (Sampling) ## Chain 2 Iteration: 1300 / 10000 [ 13%] (Sampling) ## Chain 1 Iteration: 1700 / 10000 [ 17%] (Sampling) ## Chain 2 Iteration: 1400 / 10000 [ 14%] (Sampling) ## Chain 1 Iteration: 1800 / 10000 [ 18%] (Sampling) ## Chain 1 Iteration: 1900 / 10000 [ 19%] (Sampling) ## Chain 2 Iteration: 1500 / 10000 [ 15%] (Sampling) ## Chain 1 Iteration: 2000 / 10000 [ 20%] (Sampling) ## Chain 1 Iteration: 2100 / 10000 [ 21%] (Sampling) ## Chain 2 Iteration: 1600 / 10000 [ 16%] (Sampling) ## Chain 1 Iteration: 2200 / 10000 [ 22%] (Sampling) ## Chain 1 Iteration: 2300 / 10000 [ 23%] (Sampling) ## Chain 2 Iteration: 1700 / 10000 [ 17%] (Sampling) ## Chain 1 Iteration: 2400 / 10000 [ 24%] (Sampling) ## Chain 1 Iteration: 2500 / 10000 [ 25%] (Sampling) ## Chain 2 Iteration: 1800 / 10000 [ 18%] (Sampling) ## Chain 1 Iteration: 2600 / 10000 [ 26%] (Sampling) ## Chain 1 Iteration: 2700 / 10000 [ 27%] (Sampling) ## Chain 2 Iteration: 1900 / 10000 [ 19%] (Sampling) ## Chain 1 Iteration: 2800 / 10000 [ 28%] (Sampling) ## Chain 2 Iteration: 2000 / 10000 [ 20%] (Sampling) ## Chain 1 Iteration: 2900 / 10000 [ 29%] (Sampling) ## Chain 1 Iteration: 3000 / 10000 [ 30%] (Sampling) ## Chain 2 Iteration: 2100 / 10000 [ 21%] (Sampling) ## Chain 1 Iteration: 3100 / 10000 [ 31%] (Sampling) ## Chain 1 Iteration: 3200 / 10000 [ 32%] (Sampling) ## Chain 2 Iteration: 2200 / 10000 [ 22%] (Sampling) ## Chain 1 Iteration: 3300 / 10000 [ 33%] (Sampling) ## Chain 1 Iteration: 3400 / 10000 [ 34%] (Sampling) ## Chain 2 Iteration: 2300 / 10000 [ 23%] (Sampling) ## Chain 1 Iteration: 3500 / 10000 [ 35%] (Sampling) ## Chain 1 Iteration: 3600 / 10000 [ 36%] (Sampling) ## Chain 2 Iteration: 2400 / 10000 [ 24%] (Sampling) ## Chain 1 Iteration: 3700 / 10000 [ 37%] (Sampling) ## Chain 1 Iteration: 3800 / 10000 [ 38%] (Sampling) ## Chain 2 Iteration: 2500 / 10000 [ 25%] (Sampling) ## Chain 1 Iteration: 3900 / 10000 [ 39%] (Sampling) ## Chain 1 Iteration: 4000 / 10000 [ 40%] (Sampling) ## Chain 2 Iteration: 2600 / 10000 [ 26%] (Sampling) ## Chain 1 Iteration: 4100 / 10000 [ 41%] (Sampling) ## Chain 2 Iteration: 2700 / 10000 [ 27%] (Sampling) ## Chain 1 Iteration: 4200 / 10000 [ 42%] (Sampling) ## Chain 1 Iteration: 4300 / 10000 [ 43%] (Sampling) ## Chain 2 Iteration: 2800 / 10000 [ 28%] (Sampling) ## Chain 1 Iteration: 4400 / 10000 [ 44%] (Sampling) ## Chain 1 Iteration: 4500 / 10000 [ 45%] (Sampling) ## Chain 2 Iteration: 2900 / 10000 [ 29%] (Sampling) ## Chain 1 Iteration: 4600 / 10000 [ 46%] (Sampling) ## Chain 1 Iteration: 4700 / 10000 [ 47%] (Sampling) ## Chain 2 Iteration: 3000 / 10000 [ 30%] (Sampling) ## Chain 1 Iteration: 4800 / 10000 [ 48%] (Sampling) ## Chain 1 Iteration: 4900 / 10000 [ 49%] (Sampling) ## Chain 2 Iteration: 3100 / 10000 [ 31%] (Sampling) ## Chain 1 Iteration: 5000 / 10000 [ 50%] (Sampling) ## Chain 1 Iteration: 5100 / 10000 [ 51%] (Sampling) ## Chain 2 Iteration: 3200 / 10000 [ 32%] (Sampling) ## Chain 1 Iteration: 5200 / 10000 [ 52%] (Sampling) ## Chain 1 Iteration: 5300 / 10000 [ 53%] (Sampling) ## Chain 2 Iteration: 3300 / 10000 [ 33%] (Sampling) ## Chain 1 Iteration: 5400 / 10000 [ 54%] (Sampling) ## Chain 2 Iteration: 3400 / 10000 [ 34%] (Sampling) ## Chain 1 Iteration: 5500 / 10000 [ 55%] (Sampling) ## Chain 1 Iteration: 5600 / 10000 [ 56%] (Sampling) ## Chain 2 Iteration: 3500 / 10000 [ 35%] (Sampling) ## Chain 1 Iteration: 5700 / 10000 [ 57%] (Sampling) ## Chain 1 Iteration: 5800 / 10000 [ 58%] (Sampling) ## Chain 2 Iteration: 3600 / 10000 [ 36%] (Sampling) ## Chain 1 Iteration: 5900 / 10000 [ 59%] (Sampling) ## Chain 1 Iteration: 6000 / 10000 [ 60%] (Sampling) ## Chain 2 Iteration: 3700 / 10000 [ 37%] (Sampling) ## Chain 1 Iteration: 6100 / 10000 [ 61%] (Sampling) ## Chain 2 Iteration: 3800 / 10000 [ 38%] (Sampling) ## Chain 1 Iteration: 6200 / 10000 [ 62%] (Sampling) ## Chain 1 Iteration: 6300 / 10000 [ 63%] (Sampling) ## Chain 2 Iteration: 3900 / 10000 [ 39%] (Sampling) ## Chain 1 Iteration: 6400 / 10000 [ 64%] (Sampling) ## Chain 1 Iteration: 6500 / 10000 [ 65%] (Sampling) ## Chain 2 Iteration: 4000 / 10000 [ 40%] (Sampling) ## Chain 1 Iteration: 6600 / 10000 [ 66%] (Sampling) ## Chain 1 Iteration: 6700 / 10000 [ 67%] (Sampling) ## Chain 2 Iteration: 4100 / 10000 [ 41%] (Sampling) ## Chain 1 Iteration: 6800 / 10000 [ 68%] (Sampling) ## Chain 1 Iteration: 6900 / 10000 [ 69%] (Sampling) ## Chain 2 Iteration: 4200 / 10000 [ 42%] (Sampling) ## Chain 1 Iteration: 7000 / 10000 [ 70%] (Sampling) ## Chain 1 Iteration: 7100 / 10000 [ 71%] (Sampling) ## Chain 2 Iteration: 4300 / 10000 [ 43%] (Sampling) ## Chain 1 Iteration: 7200 / 10000 [ 72%] (Sampling) ## Chain 1 Iteration: 7300 / 10000 [ 73%] (Sampling) ## Chain 2 Iteration: 4400 / 10000 [ 44%] (Sampling) ## Chain 1 Iteration: 7400 / 10000 [ 74%] (Sampling) ## Chain 1 Iteration: 7500 / 10000 [ 75%] (Sampling) ## Chain 2 Iteration: 4500 / 10000 [ 45%] (Sampling) ## Chain 1 Iteration: 7600 / 10000 [ 76%] (Sampling) ## Chain 2 Iteration: 4600 / 10000 [ 46%] (Sampling) ## Chain 1 Iteration: 7700 / 10000 [ 77%] (Sampling) ## Chain 1 Iteration: 7800 / 10000 [ 78%] (Sampling) ## Chain 2 Iteration: 4700 / 10000 [ 47%] (Sampling) ## Chain 1 Iteration: 7900 / 10000 [ 79%] (Sampling) ## Chain 1 Iteration: 8000 / 10000 [ 80%] (Sampling) ## Chain 2 Iteration: 4800 / 10000 [ 48%] (Sampling) ## Chain 1 Iteration: 8100 / 10000 [ 81%] (Sampling) ## Chain 2 Iteration: 4900 / 10000 [ 49%] (Sampling) ## Chain 1 Iteration: 8200 / 10000 [ 82%] (Sampling) ## Chain 1 Iteration: 8300 / 10000 [ 83%] (Sampling) ## Chain 2 Iteration: 5000 / 10000 [ 50%] (Sampling) ## Chain 1 Iteration: 8400 / 10000 [ 84%] (Sampling) ## Chain 1 Iteration: 8500 / 10000 [ 85%] (Sampling) ## Chain 2 Iteration: 5100 / 10000 [ 51%] (Sampling) ## Chain 1 Iteration: 8600 / 10000 [ 86%] (Sampling) ## Chain 1 Iteration: 8700 / 10000 [ 87%] (Sampling) ## Chain 2 Iteration: 5200 / 10000 [ 52%] (Sampling) ## Chain 1 Iteration: 8800 / 10000 [ 88%] (Sampling) ## Chain 1 Iteration: 8900 / 10000 [ 89%] (Sampling) ## Chain 2 Iteration: 5300 / 10000 [ 53%] (Sampling) ## Chain 1 Iteration: 9000 / 10000 [ 90%] (Sampling) ## Chain 1 Iteration: 9100 / 10000 [ 91%] (Sampling) ## Chain 2 Iteration: 5400 / 10000 [ 54%] (Sampling) ## Chain 1 Iteration: 9200 / 10000 [ 92%] (Sampling) ## Chain 1 Iteration: 9300 / 10000 [ 93%] (Sampling) ## Chain 2 Iteration: 5500 / 10000 [ 55%] (Sampling) ## Chain 1 Iteration: 9400 / 10000 [ 94%] (Sampling) ## Chain 1 Iteration: 9500 / 10000 [ 95%] (Sampling) ## Chain 2 Iteration: 5600 / 10000 [ 56%] (Sampling) ## Chain 1 Iteration: 9600 / 10000 [ 96%] (Sampling) ## Chain 2 Iteration: 5700 / 10000 [ 57%] (Sampling) ## Chain 1 Iteration: 9700 / 10000 [ 97%] (Sampling) ## Chain 1 Iteration: 9800 / 10000 [ 98%] (Sampling) ## Chain 1 Iteration: 9900 / 10000 [ 99%] (Sampling) ## Chain 2 Iteration: 5800 / 10000 [ 58%] (Sampling) ## Chain 1 Iteration: 10000 / 10000 [100%] (Sampling) ## Chain 1 finished in 31.6 seconds. ## Chain 2 Iteration: 5900 / 10000 [ 59%] (Sampling) ## Chain 2 Iteration: 6000 / 10000 [ 60%] (Sampling) ## Chain 2 Iteration: 6100 / 10000 [ 61%] (Sampling) ## Chain 2 Iteration: 6200 / 10000 [ 62%] (Sampling) ## Chain 2 Iteration: 6300 / 10000 [ 63%] (Sampling) ## Chain 2 Iteration: 6400 / 10000 [ 64%] (Sampling) ## Chain 2 Iteration: 6500 / 10000 [ 65%] (Sampling) ## Chain 2 Iteration: 6600 / 10000 [ 66%] (Sampling) ## Chain 2 Iteration: 6700 / 10000 [ 67%] (Sampling) ## Chain 2 Iteration: 6800 / 10000 [ 68%] (Sampling) ## Chain 2 Iteration: 6900 / 10000 [ 69%] (Sampling) ## Chain 2 Iteration: 7000 / 10000 [ 70%] (Sampling) ## Chain 2 Iteration: 7100 / 10000 [ 71%] (Sampling) ## Chain 2 Iteration: 7200 / 10000 [ 72%] (Sampling) ## Chain 2 Iteration: 7300 / 10000 [ 73%] (Sampling) ## Chain 2 Iteration: 7400 / 10000 [ 74%] (Sampling) ## Chain 2 Iteration: 7500 / 10000 [ 75%] (Sampling) ## Chain 2 Iteration: 7600 / 10000 [ 76%] (Sampling) ## Chain 2 Iteration: 7700 / 10000 [ 77%] (Sampling) ## Chain 2 Iteration: 7800 / 10000 [ 78%] (Sampling) ## Chain 2 Iteration: 7900 / 10000 [ 79%] (Sampling) ## Chain 2 Iteration: 8000 / 10000 [ 80%] (Sampling) ## Chain 2 Iteration: 8100 / 10000 [ 81%] (Sampling) ## Chain 2 Iteration: 8200 / 10000 [ 82%] (Sampling) ## Chain 2 Iteration: 8300 / 10000 [ 83%] (Sampling) ## Chain 2 Iteration: 8400 / 10000 [ 84%] (Sampling) ## Chain 2 Iteration: 8500 / 10000 [ 85%] (Sampling) ## Chain 2 Iteration: 8600 / 10000 [ 86%] (Sampling) ## Chain 2 Iteration: 8700 / 10000 [ 87%] (Sampling) ## Chain 2 Iteration: 8800 / 10000 [ 88%] (Sampling) ## Chain 2 Iteration: 8900 / 10000 [ 89%] (Sampling) ## Chain 2 Iteration: 9000 / 10000 [ 90%] (Sampling) ## Chain 2 Iteration: 9100 / 10000 [ 91%] (Sampling) ## Chain 2 Iteration: 9200 / 10000 [ 92%] (Sampling) ## Chain 2 Iteration: 9300 / 10000 [ 93%] (Sampling) ## Chain 2 Iteration: 9400 / 10000 [ 94%] (Sampling) ## Chain 2 Iteration: 9500 / 10000 [ 95%] (Sampling) ## Chain 2 Iteration: 9600 / 10000 [ 96%] (Sampling) ## Chain 2 Iteration: 9700 / 10000 [ 97%] (Sampling) ## Chain 2 Iteration: 9800 / 10000 [ 98%] (Sampling) ## Chain 2 Iteration: 9900 / 10000 [ 99%] (Sampling) ## Chain 2 Iteration: 10000 / 10000 [100%] (Sampling) ## Chain 2 finished in 54.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 43.0 seconds. ## Total execution time: 54.9 seconds. summary(Articulation_student_m3) ## Family: student ## Links: mu = identity; sigma = identity; nu = identity ## Formula: ArticulationS ~ 1 + Register + (1 + Register | Subject) ## Data: d (Number of observations: 48) ## Draws: 2 chains, each with iter = 9000; warmup = 0; thin = 1; ## total post-warmup draws = 18000 ## ## Group-Level Effects: ## ~Subject (Number of levels: 24) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS ## sd(Intercept) 0.42 0.24 0.03 0.90 1.00 2719 ## sd(RegisterIDS) 0.45 0.30 0.02 1.12 1.00 2634 ## cor(Intercept,RegisterIDS) -0.06 0.43 -0.80 0.79 1.00 10781 ## Tail_ESS ## sd(Intercept) 3668 ## sd(RegisterIDS) 2582 ## cor(Intercept,RegisterIDS) 12030 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.13 0.18 -0.49 0.22 1.00 16180 13178 ## RegisterIDS -0.39 0.21 -0.79 0.03 1.00 20494 13017 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.79 0.15 0.46 1.08 1.00 2056 1127 ## nu 22.52 13.99 4.94 57.82 1.00 18367 11500 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). plot(conditional_effects(Articulation_student_m3), points = T) Let’s check the prior-posterior update plots for the student t model: #Sample the parameters of interest: Posterior_student_m3 &lt;- as_draws_df(Articulation_student_m3) #Plot the prior-posterior update plot for the intercept: ggplot(Posterior_student_m3) + geom_density(aes(prior_Intercept), fill=&quot;steelblue&quot;, color=&quot;black&quot;,alpha=0.6) + geom_density(aes(b_Intercept), fill=&quot;#FC4E07&quot;, color=&quot;black&quot;,alpha=0.6) + xlab(&#39;Intercept&#39;) + theme_classic() #Plot the prior-posterior update plot for b: ggplot(Posterior_student_m3) + geom_density(aes(prior_b), fill=&quot;steelblue&quot;, color=&quot;black&quot;,alpha=0.6) + geom_density(aes(b_RegisterIDS), fill=&quot;#FC4E07&quot;, color=&quot;black&quot;,alpha=0.6) + xlab(&#39;b&#39;) + theme_classic() #Plot the prior-posterior update plot for sd of intercepts and slopes: ggplot(Posterior_student_m3) + geom_density(aes(sd_Subject__Intercept), fill=&quot;#FC4E07&quot;, color=&quot;black&quot;,alpha=0.3) + geom_density(aes(sd_Subject__RegisterIDS), fill=&quot;#228B22&quot;, color=&quot;black&quot;,alpha=0.4) + geom_density(aes(prior_sd_Subject__RegisterIDS), fill=&quot;steelblue&quot;, color=&quot;black&quot;,alpha=0.6) + xlab(&#39;sd&#39;) + theme_classic() #Plot the prior-posterior update plot for sigma: ggplot(Posterior_student_m3) + geom_density(aes(prior_sigma), fill=&quot;steelblue&quot;, color=&quot;black&quot;,alpha=0.6) + geom_density(aes(sigma), fill=&quot;#FC4E07&quot;, color=&quot;black&quot;,alpha=0.6) + xlab(&#39;sigma&#39;) + theme_classic() #Plot the prior-posterior update plot for the correlation between varying intercepts and slopes: ggplot(Posterior_student_m3) + geom_density(aes(prior_cor_Subject), fill=&quot;steelblue&quot;, color=&quot;black&quot;,alpha=0.6) + geom_density(aes(cor_Subject__Intercept__RegisterIDS), fill=&quot;#FC4E07&quot;, color=&quot;black&quot;,alpha=0.6) + xlab(&#39;cor&#39;) + theme_classic() #Plot the prior-posterior update plot for the correlation between varying intercepts and slopes: ggplot(Posterior_student_m3) + geom_density(aes(prior_nu), fill=&quot;steelblue&quot;, color=&quot;black&quot;,alpha=0.6) + geom_density(aes(nu), fill=&quot;#FC4E07&quot;, color=&quot;black&quot;,alpha=0.6) + xlab(&#39;nu&#39;) + theme_classic() How do the estimates and evidence ratio of the student model compare to that of the Gaussian model? #hypothesis check for Gaussian model: hypothesis(Articulation_m3, &quot;RegisterIDS &lt; 0&quot;) ## Hypothesis Tests for class b: ## Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob ## 1 (RegisterIDS) &lt; 0 -0.39 0.21 -0.72 -0.05 29.65 0.97 ## Star ## 1 * ## --- ## &#39;CI&#39;: 90%-CI for one-sided and 95%-CI for two-sided hypotheses. ## &#39;*&#39;: For one-sided hypotheses, the posterior probability exceeds 95%; ## for two-sided hypotheses, the value tested against lies outside the 95%-CI. ## Posterior probabilities of point hypotheses assume equal prior probabilities. #hypothesis check for student t model: hypothesis(Articulation_student_m3, &quot;RegisterIDS &lt; 0&quot;) ## Hypothesis Tests for class b: ## Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob ## 1 (RegisterIDS) &lt; 0 -0.39 0.21 -0.72 -0.04 29.2 0.97 ## Star ## 1 * ## --- ## &#39;CI&#39;: 90%-CI for one-sided and 95%-CI for two-sided hypotheses. ## &#39;*&#39;: For one-sided hypotheses, the posterior probability exceeds 95%; ## for two-sided hypotheses, the value tested against lies outside the 95%-CI. ## Posterior probabilities of point hypotheses assume equal prior probabilities. Here is some code to plot the posterior student model estimates for each individual subject: ADS_data &lt;- spread_rvars(Articulation_student_m3, r_Subject[r_Subject, Intercept], b_Intercept, b_RegisterIDS) %&gt;% mutate(Subject_estimate = r_Subject + b_Intercept) %&gt;% mutate(Intercept = replace(Intercept, Intercept == &quot;Intercept&quot;, &quot;ADS&quot;)) %&gt;% mutate(Intercept = replace(Intercept, Intercept == &quot;RegisterIDS&quot;, &quot;IDS&quot;)) %&gt;% filter(Intercept == &quot;ADS&quot;) IDS_data &lt;- spread_rvars(Articulation_student_m3, r_Subject[r_Subject, Intercept], b_Intercept, b_RegisterIDS) %&gt;% mutate(Subject_estimate = r_Subject + b_RegisterIDS) %&gt;% mutate(Intercept = replace(Intercept, Intercept == &quot;Intercept&quot;, &quot;ADS&quot;)) %&gt;% mutate(Intercept = replace(Intercept, Intercept == &quot;RegisterIDS&quot;, &quot;IDS&quot;)) %&gt;% filter(Intercept == &quot;IDS&quot;) posterior_data_plot &lt;- rbind(ADS_data, IDS_data) %&gt;% median_qi(Subject_estimate) %&gt;% mutate(Subject = rep(unique(d$Subject), 2)) student_model_plot &lt;- ggplot() + geom_point(aes(x = Intercept, y = Subject_estimate), data = posterior_data_plot, size = 2.5, color = &quot;black&quot;, ) + geom_point(aes(x = Intercept, y = Subject_estimate, color = Subject), data = posterior_data_plot, size = 1.5) + geom_path(aes(x = Intercept, y = Subject_estimate, color = Subject, group = Subject), data = posterior_data_plot, alpha = 0.7, linetype = 1) + theme_bw() + ylim(c(-0.75, 0.3)) + xlab(&#39;Speech Style&#39;) + ylab(&#39;Effect Size&#39;) + ggtitle(&#39;Student Model&#39;) + scale_color_manual(values=viridis(n = 27)) + theme(plot.title = element_text(hjust = 0.5, size=15), legend.position = &quot;none&quot;, axis.text.x = element_text(size = 13), axis.title.x = element_text(size = 13), axis.text.y = element_text(size = 12), axis.title.y = element_text(size = 13)) Let’s compare these estimates with the multi-level Gaussian model: ADS_data &lt;- spread_rvars(Articulation_m3, r_Subject[r_Subject, Intercept], b_Intercept, b_RegisterIDS) %&gt;% mutate(Subject_estimate = r_Subject + b_Intercept) %&gt;% mutate(Intercept = replace(Intercept, Intercept == &quot;Intercept&quot;, &quot;ADS&quot;)) %&gt;% mutate(Intercept = replace(Intercept, Intercept == &quot;RegisterIDS&quot;, &quot;IDS&quot;)) %&gt;% filter(Intercept == &quot;ADS&quot;) IDS_data &lt;- spread_rvars(Articulation_m3, r_Subject[r_Subject, Intercept], b_Intercept, b_RegisterIDS) %&gt;% mutate(Subject_estimate = r_Subject + b_RegisterIDS) %&gt;% mutate(Intercept = replace(Intercept, Intercept == &quot;Intercept&quot;, &quot;ADS&quot;)) %&gt;% mutate(Intercept = replace(Intercept, Intercept == &quot;RegisterIDS&quot;, &quot;IDS&quot;)) %&gt;% filter(Intercept == &quot;IDS&quot;) posterior_data_plot &lt;- rbind(ADS_data, IDS_data) %&gt;% median_qi(Subject_estimate) %&gt;% mutate(Subject = rep(unique(d$Subject), 2)) gaussian_model_plot &lt;- ggplot() + geom_point(aes(x = Intercept, y = Subject_estimate), data = posterior_data_plot, size = 2.5, color = &quot;black&quot;, ) + geom_point(aes(x = Intercept, y = Subject_estimate, color = Subject), data = posterior_data_plot, size = 1.5) + geom_path(aes(x = Intercept, y = Subject_estimate, color = Subject, group = Subject), data = posterior_data_plot, alpha = 0.7, linetype = 1) + theme_bw() + ylim(c(-0.75, 0.3)) + xlab(&#39;Speech Style&#39;) + ylab(&#39;Effect Size&#39;) + ggtitle(&#39;Gaussian Model&#39;) + scale_color_manual(values=viridis(n = 27)) + theme(plot.title = element_text(hjust = 0.5, size=15), legend.position = &quot;none&quot;, axis.text.x = element_text(size = 13), axis.title.x = element_text(size = 13), axis.text.y = element_text(size = 12), axis.title.y = element_text(size = 13)) comparison_plot &lt;- plot_grid(gaussian_model_plot, student_model_plot, nrow = 1) comparison_plot Q7: How do the individual trajectories differ between the two models? Why do you think we see these differences? Let’s also make an ellipsis plot for the student model: ### Now the ellipsis plot ## Partial pooling df_partial &lt;- tibble( Subject = rownames(coef(Articulation_student_m3)[[&quot;Subject&quot;]][,,&quot;Intercept&quot;]), ADS = coef(Articulation_student_m3)[[&quot;Subject&quot;]][,,&quot;Intercept&quot;][,1], RegisterIDS = coef(Articulation_student_m3)[[&quot;Subject&quot;]][,,&quot;RegisterIDS&quot;][,1], Type = &quot;Partial pooling&quot; ) ## Original data df_no &lt;- NULL for (s in unique(d$Subject)){ tmp &lt;- tibble( Subject = s, ADS = d$ArticulationS[d$Register==&quot;ADS&quot; &amp; d$Subject==s], RegisterIDS = d$ArticulationS[d$Register==&quot;IDS&quot; &amp; d$Subject==s] - d$ArticulationS[d$Register==&quot;ADS&quot; &amp; d$Subject==s], Type = &quot;No pooling&quot; ) if (exists(&quot;df_no&quot;)){df_no = rbind(df_no, tmp)} else {df_no = tmp} } df_total &lt;- df_no[,c(&quot;Subject&quot;)] %&gt;% mutate( ADS = mean(d$ArticulationS[d$Register==&quot;ADS&quot;]), RegisterIDS = mean(d$ArticulationS[d$Register==&quot;IDS&quot;]) - mean(d$ArticulationS[d$Register==&quot;ADS&quot;]), Type = &quot;Total pooling&quot; ) df_fixef &lt;- tibble( Type = &quot;Partial pooling (average)&quot;, ADS = fixef(Articulation_student_m3)[1], RegisterIDS = fixef(Articulation_student_m3)[2] ) # Complete pooling / fixed effects are center of gravity in the plot df_gravity &lt;- df_total %&gt;% distinct(Type, ADS, RegisterIDS) %&gt;% bind_rows(df_fixef) df_gravity ## # A tibble: 2 × 3 ## ADS RegisterIDS Type ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1.83e-16 -0.691 Total pooling ## 2 -1.34e- 1 -0.388 Partial pooling (average) df_pulled &lt;- bind_rows(df_no, df_partial) # Extract the variance covariance matrix cov_mat_t &lt;- VarCorr(Articulation_student_m3)[[&quot;Subject&quot;]]$cov cov_mat &lt;- matrix(nrow=2, ncol=2) cov_mat[1,1]&lt;-cov_mat_t[,,&quot;Intercept&quot;][1,1] cov_mat[2,1]&lt;-cov_mat_t[,,&quot;RegisterIDS&quot;][1,1] cov_mat[1,2]&lt;-cov_mat_t[,,&quot;Intercept&quot;][2,1] cov_mat[2,2]&lt;-cov_mat_t[,,&quot;RegisterIDS&quot;][2,1] cov_mat ## [,1] [,2] ## [1,] 0.23373725 -0.03978625 ## [2,] -0.03978625 0.29412976 make_ellipse &lt;- function(cov_mat, center, level) { ellipse(cov_mat, centre = center, level = level) %&gt;% as.data.frame() %&gt;% add_column(level = level) %&gt;% as_tibble() } center &lt;- fixef(Articulation_student_m3) levels &lt;- c(.1, .3, .5, .7, .9) # Create an ellipse dataframe for each of the levels defined # above and combine them df_ellipse &lt;- levels %&gt;% purrr::map_df(~ make_ellipse(cov_mat, center, level = .x)) %&gt;% dplyr::rename(ADS = x, RegisterIDS = y) df_ellipse ## # A tibble: 500 × 3 ## ADS RegisterIDS level ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0103 -0.226 0.1 ## 2 -0.000638 -0.214 0.1 ## 3 -0.0121 -0.203 0.1 ## 4 -0.0241 -0.193 0.1 ## 5 -0.0366 -0.183 0.1 ## 6 -0.0494 -0.175 0.1 ## 7 -0.0626 -0.167 0.1 ## 8 -0.0761 -0.160 0.1 ## 9 -0.0898 -0.154 0.1 ## 10 -0.104 -0.149 0.1 ## # … with 490 more rows Student_ellipsis &lt;- ggplot(df_pulled) + aes(x = ADS, y = RegisterIDS, color = Type) + # Draw contour lines from the distribution of effects geom_path( aes(group = level, color = NULL), data = df_ellipse, linetype = &quot;dashed&quot;, color = &quot;grey40&quot; ) + geom_point(data = df_gravity, size = 5) + geom_point(size = 2) + geom_path( aes(group = Subject, color = NULL), arrow = arrow(length = unit(.02, &quot;npc&quot;)) ) + # Use ggrepel to jitter the labels away from the points ggrepel::geom_text_repel( aes(label = Subject, color = NULL), data = df_no ) + # Don&#39;t forget 373 ggrepel::geom_text_repel( aes(label = Subject, color = NULL), data = df_partial ) + ggtitle(&quot;Topographic map of regression parameters&quot;) + xlab(&quot;Intercept estimate&quot;) + ylab(&quot;Slope estimate&quot;) + scale_color_brewer(palette = &quot;Dark2&quot;) + theme_classic() + theme(plot.title = element_text(hjust = 0.5, size = 15), legend.position = &quot;bottom&quot;, axis.title.x = element_text(size = 13), axis.text.y = element_text(size = 12), axis.text.x = element_text(size = 12), axis.title.y = element_text(size = 13), strip.background = element_rect(color=&quot;white&quot;, fill=&quot;white&quot;, size=1.5, linetype=&quot;solid&quot;)) "],["part-v-multi-level-model-based-on-informed-priors.html", "Chapter 5 Part V: Multi-level model based on informed priors 5.1 PART V:", " Chapter 5 Part V: Multi-level model based on informed priors 5.1 PART V: In this part, we’ll have a closer look at how to encode information from previous studies within the prior specifications of our models. Let’s start with the meta-analytic prior (i.e., Mean ES: 0.62, SE = 0.13) and add it to our prior specifications for the multi-level model. We do this by specifying the prior for the slope as normal(0.62, 0.13), as in the code block below: Articulation_f3 &lt;- bf(ArticulationS ~ 1 + Register + (1+Register|Subject)) meta_analytic_prior &lt;- c( prior(normal(0, 1), class = Intercept), prior(normal(0, 1), class = sd, coef = Intercept, group = Subject), prior(normal(0.62, 0.13), class = b), prior(normal(0, 1), class = sd, coef = RegisterIDS, group = Subject), prior(normal(1, 0.5), class = sigma), prior(lkj(2), class = cor)) Articulation_MAprior_m3 &lt;- brm( Articulation_f3, data = d, save_pars = save_pars(all = TRUE), family = gaussian, prior = meta_analytic_prior, #file = &quot;Articulation_MAprior_m3&quot;, sample_prior = T, iter = 10000, warmup = 1000, cores = 2, chains = 2, backend = &quot;cmdstanr&quot;, threads = threading(2), control = list( adapt_delta = 0.999, max_treedepth = 20)) pp_check(Articulation_MAprior_m3, ndraws = 50) plot(conditional_effects(Articulation_MAprior_m3), points = T) summary(Articulation_MAprior_m3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: ArticulationS ~ 1 + Register + (1 + Register | Subject) ## Data: d (Number of observations: 48) ## Draws: 2 chains, each with iter = 9000; warmup = 0; thin = 1; ## total post-warmup draws = 18000 ## ## Group-Level Effects: ## ~Subject (Number of levels: 24) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS ## sd(Intercept) 0.32 0.23 0.01 0.85 1.00 3426 ## sd(RegisterIDS) 0.63 0.39 0.03 1.45 1.00 2259 ## cor(Intercept,RegisterIDS) -0.10 0.44 -0.84 0.76 1.00 5864 ## Tail_ESS ## sd(Intercept) 3841 ## sd(RegisterIDS) 3882 ## cor(Intercept,RegisterIDS) 9645 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.46 0.20 -0.85 -0.06 1.00 6002 7123 ## RegisterIDS 0.43 0.13 0.19 0.68 1.00 15619 12784 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.99 0.17 0.64 1.31 1.00 2545 1726 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Q8: How does the slope estimate for this model with the meta-analytic prior compare to that with the skeptical prior (i.e., Articulation_m3)? Let’s try to do the same analysis for the Danish prior (i.e., Mean ES: -0.1, SE = 0.04): danish_prior &lt;- c( prior(normal(0, 1), class = Intercept), prior(normal(0, 1), class = sd, coef = Intercept, group = Subject), prior(normal(-0.1, 0.04), class = b), prior(normal(0, 1), class = sd, coef = RegisterIDS, group = Subject), prior(normal(1, 0.5), class = sigma), prior(lkj(2), class = cor)) Articulation_Danishprior_m3 &lt;- brm( Articulation_f3, data = d, save_pars = save_pars(all = TRUE), family = gaussian, prior = danish_prior, #file = &quot;Articulation_Danishprior_m3&quot;, sample_prior = T, iter = 10000, warmup = 1000, cores = 2, chains = 2, backend = &quot;cmdstanr&quot;, threads = threading(2), control = list( adapt_delta = 0.999, max_treedepth = 20)) pp_check(Articulation_Danishprior_m3, ndraws = 50) plot(conditional_effects(Articulation_Danishprior_m3), points = T) summary(Articulation_Danishprior_m3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: ArticulationS ~ 1 + Register + (1 + Register | Subject) ## Data: d (Number of observations: 48) ## Draws: 2 chains, each with iter = 9000; warmup = 0; thin = 1; ## total post-warmup draws = 18000 ## ## Group-Level Effects: ## ~Subject (Number of levels: 24) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS ## sd(Intercept) 0.38 0.23 0.02 0.88 1.00 2622 ## sd(RegisterIDS) 0.47 0.31 0.02 1.16 1.00 2706 ## cor(Intercept,RegisterIDS) -0.05 0.43 -0.80 0.78 1.00 12702 ## Tail_ESS ## sd(Intercept) 3422 ## sd(RegisterIDS) 2200 ## cor(Intercept,RegisterIDS) 12617 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.24 0.17 -0.57 0.09 1.00 11397 10223 ## RegisterIDS -0.11 0.04 -0.19 -0.03 1.00 28793 13780 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.88 0.15 0.56 1.15 1.00 2182 1021 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Q9: How does the slope estimate for this model with the Danish prior compare to that with the skeptical prior (i.e., Articulation_m3)? Let’s run the following code to extract and visualise the prior and posterior distributions from the different models: danish_prior_posterior &lt;- as_draws_df(Articulation_Danishprior_m3) %&gt;% mutate(priors = &quot;Danish&quot;) %&gt;% select(prior_b, b_RegisterIDS, priors) ## Warning: Dropping &#39;draws_df&#39; class as required metadata was removed. ma_prior_posterior &lt;- as_draws_df(Articulation_MAprior_m3) %&gt;% mutate(priors = &quot;MA estimates&quot;) %&gt;% select(prior_b, b_RegisterIDS, priors) ## Warning: Dropping &#39;draws_df&#39; class as required metadata was removed. skeptical_prior_posterior &lt;- as_draws_df(Articulation_m3) %&gt;% mutate(priors = &quot;Skeptical estimates&quot;) %&gt;% select(prior_b, b_RegisterIDS, priors) ## Warning: Dropping &#39;draws_df&#39; class as required metadata was removed. Posterior &lt;- rbind(danish_prior_posterior, ma_prior_posterior, skeptical_prior_posterior) plot1 &lt;- ggplot(Posterior) + theme_classic() + ggtitle(&quot;Priors&quot;) + geom_density(aes(x = prior_b, fill = priors), alpha = 0.7) + xlim(c(-1.5, 1.5)) + geom_vline(xintercept = 0.0, linetype = 3) + scale_fill_manual(name = &quot;Prior&quot;, labels = c(&#39;Meta-analytic&#39;, &quot;Danish&quot;, &quot;Skeptical&quot;), values=c(&quot;#FC4E07&quot;, &quot;steelblue&quot;, &quot;#228B22&quot;)) + theme(plot.title = element_text(hjust = 0.5, size=15), axis.ticks.y = element_blank(), axis.text.y = element_blank(), axis.title.y=element_blank(), axis.line.x = element_line(size=0.1), axis.line.y = element_line(size=0.0), axis.text.x = element_blank(), axis.title.x=element_blank(), legend.position = &quot;none&quot;) plot2 &lt;- ggplot(Posterior) + theme_classic() + xlim(c(-1.5, 1.5)) + geom_density(aes(x = b_RegisterIDS, fill = priors), alpha = 0.7) + xlab(&#39;Effect Size&#39;) + ggtitle(expression(paste(&quot;Posteriors&quot;))) + geom_vline(xintercept = 0.0, linetype = 3) + scale_fill_manual(name = &quot;Priors:&quot;, labels = c(&#39;Danish&#39;, &#39;Meta-analytic&#39;, &quot;Skeptical&quot;), values=c(&quot;#FC4E07&quot;, &quot;steelblue&quot;, &quot;#228B22&quot;)) + scale_color_manual(values=c(&quot;#FC4E07&quot;,&quot;#228B22&quot;, &quot;steelblue&quot;)) + theme(plot.title = element_text(hjust = 0.5, size=15), axis.ticks.y = element_blank(), axis.text.y = element_blank(), axis.title.y=element_blank(), axis.line = element_line(size=0), legend.position = &quot;bottom&quot;) priors_posteriors_plot &lt;- plot_grid(plot1, plot2, ncol=1) priors_posteriors_plot Q10: Why does the model with the Danish study prior not shift as much as those with the skeptical and meta-analytic priors? Answer: Q11: What does this tell us about the nature of evidence accumulation in Bayesian models? Answer: "],["recommendations-for-further-study.html", "Chapter 6 Recommendations for Further Study", " Chapter 6 Recommendations for Further Study "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
