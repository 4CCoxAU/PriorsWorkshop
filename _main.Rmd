--- 
title: "Workshop on Bayesian Inference: Priors and workflow"
author: "Riccardo Fusaroli & Chris Cox"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
# url: your book url like https://bookdown.org/yihui/bookdown
# cover-image: path to the social sharing image like images/cover.jpg
description: |
  This site provides the materials for a workshop on Bayesian data analysis, with a focus on a rigorous workflow and the definition of priors. 
  
link-citations: yes
github-repo: rstudio/bookdown-demo

---

# Introduction

Here you can find the video lectures and exercises for our 2022 workshop on Bayesian inference. The workshop assumes a minimal statistical literacy (e.g. having used t-tests and correlations). The workshop is specifically aimed to Bayes-curious students and researchers worried about having to set priors and/or other supposedly subjective choices in Bayesian analyses. Accordingly the workshop focuses on setting up a rigorous Bayesian workflow for your analysis and provides much discussion of how to define and assess priors.

The lectures are recordings of a workshop given on January 6th-7th 2022 to the Embodied Computation Group and the Body, Pain & Perception Lab at Aarhus University.

The materials change every time we teach the workshop as we keep learning, so *any feedback is very welcome*. If you notice mistakes, imprecisions, or have suggestions for improvement, add an issue to github (https://github.com/4CCoxAU/PriorsWorkshop) or send us a mail :-)

The code relies on R, Rstudio, Brms and Stan. 

The instructors are:

- Christopher Cox is a Phd student at Aarhus University and University of York, researching how infants learn their first language, with a special interest in their active and interactive role in shaping their linguistic environment. While less visible in the videos, Chris prepared all the materials and website and should receive most of the kudos!

- Riccardo Fusaroli (https://pure.au.dk/portal/da/fusaroli@cc.au.dk), an associate professor in Cognitive Science at Aarhus University, researching how social interactions work and fail, and trying to figure out how we can build better cumulative and self-critical scientific approaches.


## Structure of this Workshop

After an introduction to the rationale behind this workshop, the video lectures showcase and explain a step-by-step Bayesian workflow in setting up a concrete Bayesian analysis (of whether native speakers of Danish speak more or less clearly to their children compared to adults). The actual content of the course is organized in 4 separate sections:

-   1. Modeling your outcome: Intercepts-only Model;
-   2. Introducing a predictor: Simple Linear Regression;
-   3. Acknowledging repeated measures: Multi-level Modelling;
-   4. Acknowledging previous findings: Comparative Assessment of Informed Priors.

For each section, you will find a video lecture (building on the previous sections) and a step-by-step commented script showing how to implement the conceptual steps from the lecture in R/brms/Stan.

We strongly recommend you watch a video, download the markdown (and data) and go through the code, fitting the models, and answering the questions, instead of just quickly browsing the webpage with the code and output.

 
## Preparation
Here is what you will need for this workshop:

- up-to-date R (version 4 or above) and Rstudio (version 1.3 or above) installed and working. See here for a more detailed instruction on how to install R and Rstudio: https://happygitwithr.com/install-r-rstudio.html

- the “brms” package installed: https://github.com/paul-buerkner/brms N.B. it's not always as simple as doing an install.packages("brms"), so do follow the linked guide!

- If you are already comfortable with using R and stats, we also recommend you install the “cmdstanr” package (which makes fitting models go faster, but is not necessary for the course, nor easy to install): https://mc-stan.org/cmdstanr/articles/cmdstanr.html N.B. it's not always as simple as doing an install.packages("cmdstanr"), so do follow the linked guide!

Without these packages working, you will not be able to tackle the following practical exercises, so do install them before you move to the next section and make sure there are no errors or worrying warnings.

Once your computer is ready, you should also get your brain ready. This workshop focuses on how to do Bayesian data analysis and does not go into the details of Bayes' theorem. If you are not familiar with the theorem or need a quick refresh, we strongly recommend you give this 15 min video a watch before the workshop. This should make talk of priors and posteriors much easier to parse.  https://www.youtube.com/watch?v=HZGCoVF3YvM


Before starting with the full-on content on Bayesian Data analysis, we recommend you also watch the following introductory videos to the workshop.

## Introductory Video
This first video provides an introduction to the course instructors and outlines the structure of the workshop:

https://youtu.be/_7hlAJ6eWI4

## Video on Bayesian inference
This second video starts with a soft introduction to the basic concepts of Bayesian inference, tackles some common worries, and provides an overview of the data analysed throughout the course:

https://youtu.be/SIgxZ-u1s3A



<!--chapter:end:index.Rmd-->

# Modeling your outcome: Intercepts-only Model

## Video on Intercepts-Only Model

https://youtu.be/j1yxeRqJ0es

After watching the video we recommend you download the Markdown file and go through it in Rstudio:

* https://github.com/4CCoxAU/PriorsWorkshop/blob/08ea1968db8114690b04e839502ea5909300a62f/01-intro.Rmd

Here the data

* https://github.com/4CCoxAU/PriorsWorkshop/blob/08ea1968db8114690b04e839502ea5909300a62f/vowel_space_area_data.csv

The content of the markdown is reproduced below.

## Hands-on Exercises

Let's start by loading the required packages and data:

```{r setup, include=TRUE}
#if you don't have the pacman package loaded on your computer, uncomment the next line, install pacman, and load in the required packages
#install.packages('pacman')
#load the required packages:
pacman::p_load(tidyverse, 
       glue,
       data.table,
       dplyr,
       moments,
       tidybayes,
       ggplot2,
       ggridges,
       plyr,
       ellipse,
       brms,
       cowplot,
       viridis)
```

The data is avaiable here:

```{r, echo=FALSE}
xfun::embed_file('vowel_space_area_data.csv')
```

Read in the data:

```{r}
d <- read.csv('vowel_space_area_data.csv')

glimpse(d) # see below for an explanation of the different variables

```

### Introduction to the data

Our research question concerns the hyperarticulation hypothesis of infant-directed speech. Do caregivers produce more peripheral vowels in infant-directed speech (IDS) compared to adult-directed speech (ADS)? To answer this question, we manually analysed 9267 extracted vowel tokens from individual caregivers' IDS and ADS. In order eliminate inter-individual differences in formant values that occur due to physiological characteristics, we first z-score-normalised the formant data according to participant and then computed the total area of each caregiver's ADS and IDS vowel space. We used this measure to calculate a standardised mean difference score: Cohen's d.
    
The dataset we will analyse here consists of the following information:

* i)   Subject (i.e. unique participant pseudonym),
* ii)  Register (i.e. the speech style used - adult-directed speech (ADS) vs. infant-directed speech (IDS)),
* iii) ArticulationS (i.e. the standardized dependent variable),
* iv)  ChildSex (i.e. male vs. female), 
* v)   ChildAge (i.e. from 11 m to 24 m), 
* vi)  First_child (whether this is the caregiver's first child).

Note that, as detailed in the video, ArticulationS is a standardized variable, that is, we centered it (so that the mean is 0) and divided it by the standard deviation (so that the standard deviation is 1). Standardizing the outcome variable often (but not always!) makes setting priors easier, since we know much about which standardized effect sizes we should expect.

### Weakly Informative Priors

Let's start by building an intercepts-only model with ArticulationS as our dependent variable. We'll use the function bf() to specify the structure of this first model as follows:

```{r}
# Articulation_f1 indicates our first formula to model Articulation
Articulation_f1 <- bf(ArticulationS ~ 1)
```

Now that we have our model formula in place, we can use the function get_prior() to tell us which priors need to be specified for this model structure. This function also provides suggestions for default priors:

```{r}
get_prior(Articulation_f1, 
          data = d, 
          family = gaussian)
```

The output of the above function tells us that we need to specify a prior for the intercept (i.e. 'Intercept') and the residual variation (i.e. 'sigma'). This output also provides suggestions for priors (e.g., student_t(3, -0.2, 2.5) for the Intercept), but let's ignore those for now. 

Let's start by specifying weakly informative Gaussian priors for the intercept and sigma, centered at 0 with a standard deviation of 10. These priors allow for a large range of values for the intercept and residual variation (e.g. with this prior, we are specifying that we expect 95 % of the distribution to be between -20 and 20). The tails of these distributions thus represent very large (and unrealistic) values for our parameters of interest - but let's see where these weakly informative priors take us.

```{r}
weakly_informative_priors <- c(
  prior(normal(0, 10), class = Intercept),
  prior(normal(0, 10), class = sigma))
```

Before building an actual model of the hyperarticulation data, we can first run a model to determine whether our prior expectations generate appropriate values. This is known as a prior predictive check. The following code samples from the prior distributions we have specified above, without taking the actual data into account: 


```{r, results="hide", message=FALSE}
#I've provided comments on the parameters that were not introduced to you in the lecture, 
#so you have an idea of what they do:
Articulation_m1_prior0 <- 
  brm(
    Articulation_f1, 
    data = d,
    family = gaussian,
    prior = weakly_informative_priors,
    sample_prior = "only", #this term specifies that the model only samples from the 
                           #priors specified. 
    
    file = "Articulation_m0_prior0", #this parameter saves the model in your working 
                                      #directory to allow you to load the model at a later point.
    refit = "on_change", # this parameter checks whether you fit the model already. If you have and nothing changed since, it just loads the model. If anything (data, formula, priors) changed, the model is refit
    iter = 5000, #this iter parameter specifies the total number of iterations of the 
                 #Markov Chain Monte Carlo (MCMC) algorithm. We run prior predictive checks 
                 #with a small number of iterations as we are not interested in the statistical 
                 #results - we simply use these models to plot samples from our specified priors. 
    warmup = 1000, #this parameter specifies number of warmup iterations. Warmup iterations are used to "learn" the optimal hyper-parameters (settings) to explore the solutions (posterior space) to your problem
    cores = 2, # Number of CPU cores to use when executing the chains - 2 should be more than 
               # enough, but you can increase this if the models are taking too long to run.
    chains = 2, # Number of Markov chains (defaults to 4).
    backend = "cmdstanr", #feel free to uncomment this and the following line if you have 
                           #installed the “cmdstanr” package - runtime will be a little faster.
    threads = threading(2), 
    control = list(adapt_delta = 0.99, max_treedepth = 20)) #these parameters control the MCMC 
                                                            #sampler’s behaviour in various ways 
                                                            #(e.g., step size). You can leave them like this for now.
```

Let's plot some predictions from our priors by using the following pp_check() function. This plot shows the predictions from our prior distributions in blue and the actual data in black. This allows us to check whether our specifications of prior distributions are appropriate.

```{r}
pp_check(Articulation_m1_prior0, ndraws = 100)
```

Run the same pp_check() function above a couple of times and observe the results. You'll notice that the prior predictive plots vary each time you run the code; that's because the pp_check() function takes random samples from the model we specified above. It's therefore good practice to run the pp_check() function a couple of times to make sure that the plots show a pattern in the prior predictions of the model. 

Let's compare the predictions from our priors (in blue) with those of the data (in black); the predictions appear to have a much larger (and unrealistic) range than that shown by the data. Before moving on, try to answer the following question: 

Q1: How should we modify our priors to obtain more realistic predictions from our prior distributions?

Answer:


__________________________________________________________________________________________________________________________________________

### More Informative Priors

As mentioned above, by specifying the standard deviations of the above priors as 10, we are saying that we expect approximately 95% of the distribution of standardized measures (i.e. ArticulationS) to be between -20 and 20 sds from the mean; however, given our prior knowledge about the measure, a distance of 20 sds from the mean would be extremely unrealistic. Let's try to capture our prior knowledge of the measure in our specification of the priors.

What would be a reasonable ArticulationS value to expect given our prior knowledge about effect sizes? Effect sizes are usually distributed around ± 2 - to capture this, we can specify a standard deviation of 1 (i.e. with this, we expect approximately 95 % of the distribution to fall within ± 2 x SD ). For the standard deviation of the residual variation, sigma, we expect this to be on the same scale, and as discussed in the slides, we can specify a mean of 1 and standard deviation of 0.5:

```{r}
more_informative_priors <- c(
  prior(normal(0, 1), class = Intercept),
  prior(normal(1, 0.5), class = sigma))
```

Similar to before, then, let's run a prior predictive check to determine whether these priors provide more appropriate values:

```{r, results="hide", message=FALSE}
Articulation_m1_prior <- 
  brm(
    Articulation_f1, 
    data = d,
    family = gaussian,
    prior = more_informative_priors,
    sample_prior = "only", 
    iter = 5000,
    warmup = 1000,
    backend = "cmdstanr", 
    threads = threading(2),
    cores = 2,
    chains = 2,
    file = "Articulation_m1_prior",
    refit = "on_change",
    control = list(adapt_delta = 0.99, max_treedepth = 20))
```

Q2: Before you take a look at the prior predictive check plots, how do you expect this model to differ from that of the first and why? 

Answer:



__________________________________________________________________________________________________________________________________________

```{r}
# Prior predictive check for the weakly informative priors:
pp_check(Articulation_m1_prior0, ndraws = 100)
# Prior predictive check for the more informative priors:
pp_check(Articulation_m1_prior, ndraws = 100)
```

### Intercepts-Only Model

Now that our priors appear to be within the order of magnitude of ArticulationS that we expect (i.e. not 10 times more than the range we expect), let's use these priors to build an intercepts-only model for the actual data:

```{r, results="hide", message=FALSE}
Articulation_m1 <- 
  brm(
    Articulation_f1,
    data = d,
    save_pars = save_pars(all = TRUE),
    family = gaussian,
    prior = more_informative_priors,
    file = "Articulation_m1",
    refit = "on_change",
    sample_prior = T,
    iter = 5000,
    warmup = 1000,
    cores = 2,
    chains = 2,
    backend = "cmdstanr",
    threads = threading(2),
    control = list(
      adapt_delta = 0.99,
      max_treedepth = 15 ))
```

Now that we have our first model, we can conduct posterior predictive checks to make sure that our model has captured the data.

```{r}
pp_check(Articulation_m1, ndraws = 100)
```

The plot indicates that our model captures the overall distribution of our dependent variable (although there is a lot of uncertainty in model predictions).

Another way to ensure that our model represents a good fit to the data is to plot prior-posterior update plots. These plots show how our model updates from our priors after seeing the data. To plot these data, we'll use as_draws_df() to sample from the prior and posterior distributions for the relevant parameters (i.e. 'Intercept' and 'sigma' in this case) from the above model.

### Prior-Posterior Update Plots

```{r}
#overview of model parameters:
variables(Articulation_m1)
#Sample the parameters of interest:
Posterior_m1 <- as_draws_df(Articulation_m1)
#Plot the prior-posterior update plot for the intercept:
ggplot(Posterior_m1) +
  geom_density(aes(prior_Intercept), fill="steelblue", color="black",alpha=0.6) +
  geom_density(aes(b_Intercept), fill="#FC4E07", color="black",alpha=0.6) + 
  theme_classic()
#Plot the prior-posterior update plot for sigma:
ggplot(Posterior_m1) +
  geom_density(aes(prior_sigma), fill="steelblue", color="black",alpha=0.6) +
  geom_density(aes(sigma), fill="#FC4E07", color="black",alpha=0.6) + 
  theme_classic()
```

These prior-posterior update plots (with the prior in blue and the posterior in orange) indicate that our model has learned after seeing the data - great stuff! To check your intuitions, try to answer the following question:

Q3: How do you think these prior-posterior update plots would compare with those of the weakly informative priors we specified above (repeated below)?

weakly_informative_priors <- c(
  prior(normal(0, 10), class = Intercept),
  prior(normal(0, 10), class = sigma))

Answer:



__________________________________________________________________________________________________________________________________________


### Interpreting Model Output
Now that we have checked (via posterior predictive checks and prior-posterior update plots) that our model captures relevant information, let's have a look at the model output:

```{r}
summary(Articulation_m1)
```

The model output gives a summary of the model parameters and then provides estimates of 'Population-level Effects' and 'Family Specific Parameters'. Let's go through the 'Population-level Effects' and 'Family Specific Parameters' from top to bottom and left to right:

  - The Estimate(s) obtained for the Intercept is centered on approx. -0.34 and has an estimated error of 0.15. The 95 % credible interval is     
    approximately [-.63; -.04]). 
  - In the 'Family-Specific Parameters' section, the residual variation (i.e., sigma) is estimated to be approximately 1.02 [0.85; 1.25].
  - The Rhat values provide information about the convergence of the algorithm. Rhat values close to 1 suggest that the model has converged.
  - The Bulk_ESS and Tail_ESS (effective sample size (ESS)) capture the sampling efficiency in the bulk and tails of the distribution.



### Bonus Question

Try to adapt the code for Articulation_m0 and run a model with the weakly-informative priors:

```{r}


```

<!--chapter:end:01-intro.Rmd-->

# Simple Linear Regression

## Video on Simple Linear Regression:

https://youtu.be/7tGixoOdW5U

After watching the video we recommend you download the Markdown file and go through it in Rstudio:

* https://github.com/4CCoxAU/PriorsWorkshop/blob/08ea1968db8114690b04e839502ea5909300a62f/02-part1.Rmd

Here the data (same data as the previous markdown)

* https://github.com/4CCoxAU/PriorsWorkshop/blob/08ea1968db8114690b04e839502ea5909300a62f/vowel_space_area_data.csv

The content of the markdown is reproduced below.

## Hands-on Exercises

### Prior Predictive Checks

Let's start off by building a model that includes the constant effect of Register by using the bf() function:

```{r}
Articulation_f2 <- bf(ArticulationS ~ 1 + Register)
```

Let's see which priors we need to specify for this model by using the same get_prior() function as in Part I:

```{r}
get_prior(Articulation_f2,
          data = d, 
          family = gaussian)
```

The output lets us know that we need to specify priors for the Intercept, slope (i.e. b), and residual variation (i.e. sigma). Let's use the priors we discussed in the lecture:

```{r}
Articulation_p2 <- c(
  prior(normal(0, 1), class = Intercept),
  prior(normal(0, 0.3), class = b),
  prior(normal(1, 0.5), class = sigma))
```

Similar to before, let's perform a prior predictive check to make sure that our prior expectations generate relevant values:

```{r, results="hide", message=FALSE}
Articulation_m2_prior <-
  brm(
    Articulation_f2,
    data = d,
    save_pars = save_pars(all = TRUE),
    family = gaussian,
    prior = Articulation_p2,
    file = "Articulation_m2_prior",
    refit = "on_change",
    sample_prior = "only",
    iter = 5000, 
    warmup = 1000,
    cores = 2,
    chains = 2,
    backend = "cmdstanr",
    threads = threading(2),
    control = list(
      adapt_delta = 0.99,
      max_treedepth = 15 ))
```

Let's run the pp_check() function a couple of times to check the prior predictions:

```{r}
pp_check(Articulation_m2_prior, ndraws = 100)
```

### Simple Regression Model
Great! The samples from our priors appear to be within the order of magnitude that we expect. Let's run the model with these priors on the actual data:

```{r, results="hide", message=FALSE}
Articulation_m2 <-
  brm(
    Articulation_f2,
    data = d,
    save_pars = save_pars(all = TRUE),
    family = gaussian,
    prior = Articulation_p2,
    file = "Articulation_m2",
    refit = "on_change",
    sample_prior = T,
    iter = 5000, 
    warmup = 1000,
    cores = 2,
    chains = 2,
    backend = "cmdstanr",
    threads = threading(2),
    control = list(
      adapt_delta = 0.99,
      max_treedepth = 15 ))
```

Let's make sure that the model has captured the data by running the pp_check() function a couple of times:

```{r}
pp_check(Articulation_m2, ndraws = 100)
```

These plots indicate that that the posterior predictions from our model are comparable to the observed data. Perfect! Now that our model includes the constant effect of Register, we can create a plot to explore how Register changes the dependent variable (i.e. ArticulationS). We do this with the conditional_effects() function:

```{r}
# Model inference (population estimate)
plot(conditional_effects(Articulation_m2))

# Model inference (population estimate) plus actual data
plot(conditional_effects(Articulation_m2), points = T)

# Model predictions (population estimate + sigma)
plot(conditional_effects(Articulation_m2, spaghetti = T, method = "predict"), points = T)
```

Q3.5: how do the plots using the "method="predict"" differ from the first two?

Answer:


__________________________________________________________________________________________________________________________________________

### Prior-Posterior Update Plots
Let's have a look at the prior-posterior update plots for this model: 

```{r}
#Sample the parameters of interest:
Posterior_m2 <- as_draws_df(Articulation_m2)

#Plot the prior-posterior update plot for the intercept:
ggplot(Posterior_m2) +
  geom_density(aes(prior_Intercept), fill="steelblue", color="black",alpha=0.6) +
  geom_density(aes(b_Intercept), fill="#FC4E07", color="black",alpha=0.6) + 
  xlab('Intercept') +
  theme_classic()

#Plot the prior-posterior update plot for b:
ggplot(Posterior_m2) +
  geom_density(aes(prior_b), fill="steelblue", color="black",alpha=0.6) +
  geom_density(aes(b_RegisterIDS), fill="#FC4E07", color="black",alpha=0.6) + 
  xlab('b') +
  theme_classic()

#Plot the prior-posterior update plot for sigma:
ggplot(Posterior_m2) +
  geom_density(aes(prior_sigma), fill="steelblue", color="black",alpha=0.6) +
  geom_density(aes(sigma), fill="#FC4E07", color="black",alpha=0.6) + 
  xlab('Sigma') +
  theme_classic()
```

Let's try to read the model output and interpret the findings:

```{r}
summary(Articulation_m2)
```

Q4: What does this model suggest about the hyperarticulation hypothesis in Danish?

Answer:



__________________________________________________________________________________________________________________________________________

### Evidence Ratio
Let's test our hypothesis that caregivers' vowel spaces are smaller in IDS by calculating the evidence ratio, as discussed in the lecture, using the hypothesis() function:

```{r}
hypothesis(Articulation_m2, "RegisterIDS < 0")
```

### Prior Sensitivity Check
The evidence ratio of the above constant-effect model appears to provide quite strong evidence in favour of our hypothesis. If you're a worrier like us, however, you'll start to second-guess this result and think something like the following: how can we check the extent to which our specification of priors has influenced the estimates?

The answer is to conduct a prior robustness check; that is, we can loop through 15 priors with 15 different standard deviations and observe the effect it has on our posterior estimate. We'll do this in a for loop to make it easy to see what's going on:


```{r, results="hide", message=FALSE}
# The priors for the above model, repeated here:
Articulation_p2 <- c(
  prior(normal(0, 1), class = Intercept),
  prior(normal(0, 0.3), class = b),
  prior(normal(1, 0.5), class = sigma))
# construct a sequence of sds to loop through for the slope prior:
priSD <- seq(0.1, 1.5, length.out = 15)
priorsN <- Articulation_p2
#create empty variables to store output of the loop:
post_pred <- c()
post_pred_lci <- c()
post_pred_uci <- c()

for (i in 1:length(priSD)) {
  priorsN[2,] <- set_prior(paste0("normal(0, ", priSD[i],")"), class = "b")
  model_for_loop <- brm(Articulation_f2,
                                     data   = d, 
                                     family = gaussian, 
                                     prior  = priorsN, 
                                     sample_prior = T,
                                     warmup = 1000, 
                                     iter   = 5000, 
                                     cores  = 2,
                                     chains = 2,
                                     backend = "cmdstanr",
                                     threads = threading(2),
                                     save_pars = save_pars(all = TRUE),
                                     control = list(adapt_delta   = 0.99, 
                                                    max_treedepth = 15))
  
  post_preds <- spread_draws(model_for_loop, b_RegisterIDS)
  post_pred[i] <- median(post_preds$b_RegisterIDS)
  post_pred_lci[i] <- quantile(post_preds$b_RegisterIDS, prob = 0.025)
  post_pred_uci[i] <- quantile(post_preds$b_RegisterIDS, prob = 0.975)
}

models_data <- tibble(priSD, post_pred, post_pred_lci, post_pred_uci)
ggplot(data=models_data, aes(x=priSD, y=post_pred)) +
  geom_point(size = 3) +
  geom_pointrange(ymin = post_pred_lci, ymax = post_pred_uci) +
  ylim(-1.3, 0.3) +
  labs(x="Standard Deviation of Slope Prior", 
       y="Posterior Estimate for slope", 
       title="Sensitivity analysis for constant-effect model") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5, size = 15),
        axis.title.x = element_text(size = 13),
        axis.text.y = element_text(size = 12),
        axis.text.x = element_text(size = 12),
        axis.title.y = element_text(size = 13))
```

### Bonus Content on Heteroskedasticity

Now that we've calmed our worries about the influence of our priors, we can feel slightly more confident in our results. But what if we're also worried about the influence of heteroskedasticity (beyond how to pronounce it)? Don't worry - we can simply model the variance in the two groups and check whether it changes the results (and better describes the data).

```{r, results="hide", message=FALSE}
bonus_model <- bf(ArticulationS ~ 1 + Register, sigma ~ 1 + Register)

get_prior(bonus_model, 
          data = d, 
          family = gaussian)
bonus_priors <- c(
  prior(normal(0, 1), class = Intercept),
  prior(normal(0, 0.3), class = b),
  prior(normal(1, 0.5), class = Intercept, dpar=sigma),
  prior(normal(0, 1), class = b, dpar=sigma)
  )

bonus_model_prior <-
  brm(
    bonus_model,
    data = d,
    save_pars = save_pars(all = TRUE),
    family = gaussian,
    prior = bonus_priors,
    file = "bonus_model_prior",
    refit = "on_change",
    sample_prior = "only",
    iter = 2000,
    cores = 2,
    chains = 2,
    backend = "cmdstanr",
    threads = threading(2),
    control = list(
      adapt_delta = 0.99,
      max_treedepth = 15 ))


pp_check(bonus_model_prior, ndraws=50)

bonus_model_m <-
  brm(
    bonus_model,
    data = d,
    save_pars = save_pars(all = TRUE),
    family = gaussian,
    prior = bonus_priors,
    file = "bonus_model_m",
    refit = "on_change",
    sample_prior = T,
    iter = 2000, 
    cores = 2,
    chains = 2,
    backend = "cmdstanr",
    threads = threading(2),
    control = list(
      adapt_delta = 0.99,
      max_treedepth = 15 ))
pp_check(bonus_model_m, ndraws=50)
plot(conditional_effects(bonus_model_m), points = T)

#let's compare this to the other model:
plot(conditional_effects(Articulation_m2), points = T)
```

```{r}
summary(bonus_model_m)
```

Note: we have not checked our priors here. That's an exercise for you!

<!--chapter:end:02-part1.Rmd-->

# Multi-level model

## Videos on Multi-level model:

https://youtu.be/yUs4LB_9KWw

https://youtu.be/A1osWXChYr8

After watching the videos we recommend you download the Markdown file and go through it in Rstudio:

* https://github.com/4CCoxAU/PriorsWorkshop/blob/08ea1968db8114690b04e839502ea5909300a62f/03-part2.Rmd

Here the data (same data as the previous markdown)

* https://github.com/4CCoxAU/PriorsWorkshop/blob/08ea1968db8114690b04e839502ea5909300a62f/vowel_space_area_data.csv

The content of the markdown is reproduced below.

## Hands-on Exercises

### Prior Specification

Let's take our analysis to the next level (pun intended) by making a multi-level model with varying intercepts and varying slopes. In this model, we assign unique intercepts and slopes for each subject and assign them a common prior distribution. Let's check which priors we need to specify for this model:

```{r}
Articulation_f3 <- bf(ArticulationS ~ 1 + Register + (1+Register|Subject))
get_prior(Articulation_f3,
          data = d, 
          family = gaussian)
```

The output of the above function tells us that we now have three sources of variation in the model: 

* i) the usual standard deviation of the residuals (i.e., 'sigma'), 
* ii) the standard deviation of the population of by-subject varying intercepts (i.e., 'Intercept'), and 
* iii) the standard deviation of the population of by-subject varying slopes (i.e., 'RegisterIDS'). 

The latter two sources of variation provide one of the most essential features of multi-level models: partial pooling, as we discussed in the lecture.

We are also told that we need to specify a prior for the correlation between the varying intercepts and the varying slopes (i.e. class = 'cor'). This correlation captures the fact that we cannot assume the intercept and slope to be entirely independent. For example, caregivers who naturally produce a small vowel space in ADS can expand their vowel space when using IDS to a greater extent and may therefore show stronger effects.

We model the correlation between varying intercepts and slopes by using a so-called LKJ prior. The basic idea of the LKJ prior is that as its parameter increases, the prior favors less extreme correlations - that is, if we specify the LKJ parameter as 1, the prior represents a uniform, uninformative distribution (similar to that of a beta(1,1) distribution). If we set the parameter to 2, on the other hand, the prior serves to dampen the likelihood of extreme correlations, and we represent our lack of knowledge about the extent of correlation between the parameters. We'll see what our prior(lkj(2), class = cor) looks like in the prior-posterior update plot below.

As usual, let's start by performing a prior predictive check:

```{r, results="hide", message=FALSE}
Articulation_p3 <- c(
  prior(normal(0, 1), class = Intercept),
  prior(normal(0, 1), class = sd, coef = Intercept, group = Subject),
  prior(normal(0, 0.3), class = b),
  prior(normal(0, 1), class = sd, coef = RegisterIDS, group = Subject),
  prior(normal(1, 0.5), class = sigma),
  prior(lkj(2), class = cor))

Articulation_m3_prior <- 
  brm(
    Articulation_f3,
    data = d,
    save_pars = save_pars(all = TRUE),
    family = gaussian,
    prior = Articulation_p3,
    file = "Articulation_m3_prior",
    refit = "on_change",
    sample_prior = "only",
    iter = 5000, 
    warmup = 1000,
    cores = 2,
    chains = 2,
    backend = "cmdstanr",
    threads = threading(2),
    control = list(
      adapt_delta = 0.999,
      max_treedepth = 20))
pp_check(Articulation_m3_prior, ndraws=100)
```

After running the pp_check() function a couple of times, we should be convinced that our priors are within the order of magnitude that we expect. Great, let's run the model on the actual data then:

### Multi-Level Model
```{r, results="hide", message=FALSE}
Articulation_m3 <- 
  brm(
    Articulation_f3,
    data = d,
    save_pars = save_pars(all = TRUE),
    family = gaussian,
    prior = Articulation_p3,
    file = "Articulation_m3",
    refit = "on_change",
    sample_prior = T,
    iter = 5000, 
    warmup = 1000,
    cores = 2,
    chains = 2,
    backend = "cmdstanr",
    threads = threading(2),
    control = list(
      adapt_delta = 0.999,
      max_treedepth = 20))
```

As usual, let's perform some posterior predictive checks to make sure that the model captures the data:

```{r}
pp_check(Articulation_m3, ndraws=100)
```


Everything looks great, so let's plot the conditional effects and compare the output of the multi-level model with that of the previous constant-effect model:

```{r}
plot(conditional_effects(Articulation_m3), points = T)
#summary of the multi-level model:
summary(Articulation_m3)
#summary of the constant-effect model:
summary(Articulation_m2)
```


Q5: How does the estimate for sigma (i.e. the residual variation) in this multi-level model differ from the simple regression model? What does this indicate?

Answer:


__________________________________________________________________________________________________________________________________________

### Prior-Posterior Update Plots

Let's create some prior-posterior update plots so we can visualise how our model updates after seeing the data:

```{r}
#Sample the parameters of interest:
Posterior_m3 <- as_draws_df(Articulation_m3)

#Plot the prior-posterior update plot for the intercept:
ggplot(Posterior_m3) +
  geom_density(aes(prior_Intercept), fill="steelblue", color="black",alpha=0.6) +
  geom_density(aes(b_Intercept), fill="#FC4E07", color="black",alpha=0.6) + 
  xlab('Intercept') +
  theme_classic()

#Plot the prior-posterior update plot for b:
ggplot(Posterior_m3) +
  geom_density(aes(prior_b), fill="steelblue", color="black",alpha=0.6) +
  geom_density(aes(b_RegisterIDS), fill="#FC4E07", color="black",alpha=0.6) + 
  xlab('b') +
  theme_classic()

#Plot the prior-posterior update plot for sd of intercepts and slopes:
ggplot(Posterior_m3) +
  geom_density(aes(sd_Subject__Intercept), fill="#FC4E07", color="black",alpha=0.3) + 
  geom_density(aes(sd_Subject__RegisterIDS), fill="#228B22", color="black",alpha=0.4) + 
  geom_density(aes(prior_sd_Subject__RegisterIDS), fill="steelblue", color="black",alpha=0.6) +
  xlab('sd') +
  theme_classic()

#Plot the prior-posterior update plot for sigma:
ggplot(Posterior_m3) +
  geom_density(aes(prior_sigma), fill="steelblue", color="black",alpha=0.6) +
  geom_density(aes(sigma), fill="#FC4E07", color="black",alpha=0.6) + 
  xlab('sigma') +
  theme_classic()

#Plot the prior-posterior update plot for the correlation between varying intercepts and slopes:
ggplot(Posterior_m3) +
  geom_density(aes(prior_cor_Subject), fill="steelblue", color="black",alpha=0.6) +
  geom_density(aes(cor_Subject__Intercept__RegisterIDS), fill="#FC4E07", color="black",alpha=0.6) + 
  xlab('cor') +
  theme_classic()
```

### Hypothesis Testing

Let's test our hypothesis with this model by using the hypothesis() function:

```{r}
#overall
hypothesis(Articulation_m3, "RegisterIDS < 0")

#for individual subjectss:
hypothesis(Articulation_m3, "RegisterIDS < 0", group = "Subject", scope="coef")
```

### Prior Sensitivity Check

These results look interesting - however, we may be worried about the influence of our priors. Let's conduct a prior robustness check for this multi-level model to calm our worries:

```{r, results="hide", message=FALSE}
#code to loop through sd of intercept prior:
priSD <- seq(0.1, 1.5, length.out = 15)
priorsN <- Articulation_p3
#create empty sets to store output of the loop:
post_pred <- c()
post_pred_lci <- c()
post_pred_uci <- c()
for (i in 1:length(priSD)) {
  priorsN[3,] <- set_prior(paste0("normal(0, ", priSD[i],")"), class = "b")
  model_for_loop <- brm(Articulation_f3,
                                     data   = d, 
                                     family = gaussian, 
                                     prior  = priorsN, 
                                     sample_prior = T,
                                     warmup = 1000, 
                                     iter   = 5000, 
                                     cores  = 2,
                                     chains = 2,
                                     backend = "cmdstanr",
                                     threads = threading(2),
                                     save_pars = save_pars(all = TRUE),
                                     control = list(adapt_delta   = 0.99, 
                                                    max_treedepth = 15))
  
  post_preds <- spread_draws(model_for_loop, b_RegisterIDS)
  post_pred[i] <- median(post_preds$b_RegisterIDS)
  post_pred_lci[i] <- quantile(post_preds$b_RegisterIDS, prob = 0.025)
  post_pred_uci[i] <- quantile(post_preds$b_RegisterIDS, prob = 0.975)
}

models_data <- data.frame(priSD, post_pred, post_pred_lci, post_pred_uci)

ggplot(data=models_data, aes(x=priSD, y=post_pred)) +
  geom_point(size = 3) +
  geom_pointrange(ymin = post_pred_lci, ymax = post_pred_uci) +
  ylim(-1.3, 0.3) +
  labs(x="Standard Deviation of Slope Prior", 
       y="Posterior Estimate for slope", 
       title="Sensitivity analysis for multi-level model") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5, size = 15),
        axis.title.x = element_text(size = 13),
        axis.text.y = element_text(size = 12),
        axis.text.x = element_text(size = 12),
        axis.title.y = element_text(size = 13))
```

Q6: How does this prior robustness check compare to that of the simple linear regression model?

Answer:



__________________________________________________________________________________________________________________________________________



### Partial Pooling

The following code shows how to extract relevant data from the models and how to plot partial pooling:

```{r}
plot_df <- tibble(
  Subject = rownames(coef(Articulation_m3)[["Subject"]][,,"Intercept"]),
  ADS = coef(Articulation_m3)[["Subject"]][,,"Intercept"][,1],
  IDS = ADS + coef(Articulation_m3)[["Subject"]][,,"RegisterIDS"][,1],
  Type = "partial pooling"
) %>% pivot_longer(ADS:IDS) %>% dplyr::rename(
  Register = name,
  ArticulationS = value
)
df <- d[, c("Subject", "Register", "ArticulationS")] %>%
  mutate(Type = "no pooling")
pool_df <- df[,c("Subject", "Register")] %>%
  mutate(
    ArticulationS = ifelse(Register=="ADS", mean(df$ArticulationS[df$Register=="ADS"]), mean(df$ArticulationS[df$Register=="IDS"])),
    Type = "total pooling"
  )

plot_df <- rbind(plot_df,df) 

plot_df <- rbind(plot_df,pool_df) 

plot_df <- plot_df %>%
  mutate(Register=as.numeric(as.factor(Register)))

ggplot(plot_df, aes(Register, ArticulationS, color = Type)) + 
  geom_path(size = 1) + 
  geom_point() + 
  facet_wrap(.~Subject) +
  scale_x_continuous(breaks=seq(1, 2, 1)) +
  theme_bw() +
  theme(axis.title.x = element_text(size = 13),
        axis.text.y = element_text(size = 12),
        axis.text.x = element_text(size = 12),
        axis.title.y = element_text(size = 13),
        strip.background = element_rect(color="white", fill="white", size=1.5, linetype="solid"),
        strip.text.x = element_text(size = 10, color = "black"))
```

The following code shows how to plot partial pooling:

```{r, warning=FALSE}
### Now the ellipsis plot
## Partial pooling
df_partial <- tibble(
  Subject = rownames(coef(Articulation_m3)[["Subject"]][,,"Intercept"]),
  ADS = coef(Articulation_m3)[["Subject"]][,,"Intercept"][,1],
  RegisterIDS = coef(Articulation_m3)[["Subject"]][,,"RegisterIDS"][,1],
  Type = "Partial pooling"
)
## Original data
df_no <- NULL
for (s in unique(d$Subject)){
  tmp <- tibble(
    Subject = s,
    ADS = d$ArticulationS[d$Register=="ADS" & d$Subject==s],
    RegisterIDS = d$ArticulationS[d$Register=="IDS" & d$Subject==s] - d$ArticulationS[d$Register=="ADS" & d$Subject==s],
    Type = "No pooling"
  )
  if (exists("df_no")){df_no = rbind(df_no, tmp)} else {df_no = tmp}
}
df_total <- df_no[,c("Subject")] %>%
  mutate(
    ADS = mean(d$ArticulationS[d$Register=="ADS"]),
    RegisterIDS =  mean(d$ArticulationS[d$Register=="IDS"]) - mean(d$ArticulationS[d$Register=="ADS"]),
    Type = "Total pooling"
  )
df_fixef <- tibble(
  Type = "Partial pooling (average)",
  ADS = fixef(Articulation_m3)[1],
  RegisterIDS = fixef(Articulation_m3)[2]
)

# Complete pooling / fixed effects are center of gravity in the plot
df_gravity <- df_total %>% 
  distinct(Type, ADS, RegisterIDS) %>% 
  bind_rows(df_fixef)

df_pulled <- bind_rows(df_no, df_partial)

# Extract the variance covariance matrix
cov_mat_t <- VarCorr(Articulation_m3)[["Subject"]]$cov
cov_mat <- matrix(nrow=2, ncol=2)
cov_mat[1,1]<-cov_mat_t[,,"Intercept"][1,1]
cov_mat[2,1]<-cov_mat_t[,,"RegisterIDS"][1,1]
cov_mat[1,2]<-cov_mat_t[,,"Intercept"][2,1]
cov_mat[2,2]<-cov_mat_t[,,"RegisterIDS"][2,1]

make_ellipse <- function(cov_mat, center, level) {
  ellipse(cov_mat, centre = center, level = level) %>%
    as.data.frame() %>%
    add_column(level = level) %>% 
    as_tibble()
}
center <- fixef(Articulation_m3)
levels <- c(.1, .3, .5, .7, .9)

# Create an ellipse dataframe for each of the levels defined 
# above and combine them
df_ellipse <- levels %>%
  purrr::map_df(~ make_ellipse(cov_mat, center, level = .x)) %>% 
  dplyr::rename(ADS = x, RegisterIDS = y)

Gaussian_ellipsis <- ggplot(df_pulled) + 
  aes(x = ADS, y = RegisterIDS, color = Type) + 
  # Draw contour lines from the distribution of effects
  geom_path(
    aes(group = level, color = NULL), 
    data = df_ellipse, 
    linetype = "dashed", 
    color = "grey40"
  ) + 
  geom_point(data = df_gravity, size = 5) + 
  geom_point(size = 2) + 
  geom_path(
    aes(group = Subject, color = NULL), 
    arrow = arrow(length = unit(.02, "npc"))
  ) + 
  # Use ggrepel to jitter the labels away from the points
  ggrepel::geom_text_repel(
    aes(label = Subject, color = NULL), 
    data = df_no
  ) + 
  # Don't forget 373
  ggrepel::geom_text_repel(
    aes(label = Subject, color = NULL), 
    data = df_partial
  ) + 
  ggtitle("Topographic map of regression parameters") + 
  xlab("Intercept estimate") + 
  ylab("Slope estimate") + 
  scale_color_brewer(palette = "Dark2") +
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5, size = 15),
        legend.position = "bottom",
        axis.title.x = element_text(size = 13),
        axis.text.y = element_text(size = 12),
        axis.text.x = element_text(size = 12),
        axis.title.y = element_text(size = 13),
        strip.background = element_rect(color="white", fill="white", size=1.5, linetype="solid"))

Gaussian_ellipsis
```


### Student t model
One of the ways to allow for more flexibility in the distribution of the data would be to model the data using a student t likelihood. The main difference between the Gaussian and student t distribution is that a student t distribution with a low degrees of freedom parameter, nu, has heavier tails than the conventional Gaussian distribution. This form of model thus dampens the influence of outliers - incorporating outliers without allowing them to dominate non-outlier data. You can specify a student likelihood in the model using the 'family' parameter. As we can see in the results of the below get_prior() function, we now have to provide an additional prior for nu. For the following student t model, we will set the prior to be gamma(2, 0.1):

```{r, results="hide", message=FALSE}
Articulation_f3 <- bf(ArticulationS ~ 1 + Register + (1+Register|Subject))

get_prior(Articulation_f3, data = d, family = student)

student_priors <- c(
  prior(normal(0, 1), class = Intercept),
  prior(normal(0, 1), class = sd, coef = Intercept, group = Subject),
  prior(normal(0, 0.3), class = b),
  prior(normal(0, 1), class = sd, coef = RegisterIDS, group = Subject),
  prior(normal(1, 0.5), class = sigma),
  prior(lkj(2), class = cor),
  prior(gamma(2, 0.1), class = nu))

Articulation_student_m3 <- 
  brm(
    Articulation_f3,
    data = d,
    save_pars = save_pars(all = TRUE),
    family = student,
    prior = student_priors,
    file = "Articulation_student_m3",
    refit = "on_change",
    sample_prior = T,
    iter = 10000, 
    warmup = 1000,
    cores = 2,
    chains = 2,
    backend = "cmdstanr",
    threads = threading(2),
    control = list(
      adapt_delta = 0.999,
      max_treedepth = 20))

summary(Articulation_student_m3)

plot(conditional_effects(Articulation_student_m3), points = T)
```

Let's check the prior-posterior update plots for the student t model:

```{r}
#Sample the parameters of interest:
Posterior_student_m3 <- as_draws_df(Articulation_student_m3)

#Plot the prior-posterior update plot for the intercept:
ggplot(Posterior_student_m3) +
  geom_density(aes(prior_Intercept), fill="steelblue", color="black",alpha=0.6) +
  geom_density(aes(b_Intercept), fill="#FC4E07", color="black",alpha=0.6) + 
  xlab('Intercept') +
  theme_classic()

#Plot the prior-posterior update plot for b:
ggplot(Posterior_student_m3) +
  geom_density(aes(prior_b), fill="steelblue", color="black",alpha=0.6) +
  geom_density(aes(b_RegisterIDS), fill="#FC4E07", color="black",alpha=0.6) + 
  xlab('b') +
  theme_classic()

#Plot the prior-posterior update plot for sd of intercepts and slopes:
ggplot(Posterior_student_m3) +
  geom_density(aes(sd_Subject__Intercept), fill="#FC4E07", color="black",alpha=0.3) + 
  geom_density(aes(sd_Subject__RegisterIDS), fill="#228B22", color="black",alpha=0.4) + 
  geom_density(aes(prior_sd_Subject__RegisterIDS), fill="steelblue", color="black",alpha=0.6) +
  xlab('sd') +
  theme_classic()

#Plot the prior-posterior update plot for sigma:
ggplot(Posterior_student_m3) +
  geom_density(aes(prior_sigma), fill="steelblue", color="black",alpha=0.6) +
  geom_density(aes(sigma), fill="#FC4E07", color="black",alpha=0.6) + 
  xlab('sigma') +
  theme_classic()

#Plot the prior-posterior update plot for the correlation between varying intercepts and slopes:
ggplot(Posterior_student_m3) +
  geom_density(aes(prior_cor_Subject), fill="steelblue", color="black",alpha=0.6) +
  geom_density(aes(cor_Subject__Intercept__RegisterIDS), fill="#FC4E07", color="black",alpha=0.6) + 
  xlab('cor') +
  theme_classic()

#Plot the prior-posterior update plot for the correlation between varying intercepts and slopes:
ggplot(Posterior_student_m3) +
  geom_density(aes(prior_nu), fill="steelblue", color="black",alpha=0.6) +
  geom_density(aes(nu), fill="#FC4E07", color="black",alpha=0.6) + 
  xlab('nu') +
  theme_classic()
```


How do the estimates and evidence ratio of the student model compare to that of the Gaussian model?

```{r}
#hypothesis check for Gaussian model:
hypothesis(Articulation_m3, "RegisterIDS < 0")

#hypothesis check for student t model:
hypothesis(Articulation_student_m3, "RegisterIDS < 0")

```


Here is some code to plot the posterior student model estimates for each individual subject:

```{r}
ADS_data <- spread_rvars(Articulation_student_m3, r_Subject[r_Subject, Intercept], b_Intercept, b_RegisterIDS) %>%
  mutate(Subject_estimate = r_Subject + b_Intercept) %>%
  mutate(Intercept = replace(Intercept, Intercept == "Intercept", "ADS")) %>%
  mutate(Intercept = replace(Intercept, Intercept == "RegisterIDS", "IDS")) %>%
  filter(Intercept == "ADS")

IDS_data <- spread_rvars(Articulation_student_m3, r_Subject[r_Subject, Intercept], b_Intercept, b_RegisterIDS) %>%
  mutate(Subject_estimate = r_Subject + b_RegisterIDS) %>%
  mutate(Intercept = replace(Intercept, Intercept == "Intercept", "ADS")) %>%
  mutate(Intercept = replace(Intercept, Intercept == "RegisterIDS", "IDS")) %>%
  filter(Intercept == "IDS")

posterior_data_plot <- rbind(ADS_data, IDS_data) %>%
  median_qi(Subject_estimate) %>%
  mutate(Subject = rep(unique(d$Subject), 2))

student_model_plot <- ggplot() +
  geom_point(aes(x = Intercept, y = Subject_estimate), data = posterior_data_plot, size = 2.5, color = "black", ) +
  geom_point(aes(x = Intercept, y = Subject_estimate, color = Subject), data = posterior_data_plot, size = 1.5) +
  geom_path(aes(x = Intercept, y = Subject_estimate, color = Subject, group = Subject), data = posterior_data_plot, alpha = 0.7, linetype = 1) +
  theme_bw() +
  ylim(c(-0.75, 0.3)) +
  xlab('Speech Style') + 
  ylab('Effect Size') +
  ggtitle('Student Model') +
  scale_color_manual(values=viridis(n = 27)) + 
  theme(plot.title = element_text(hjust = 0.5, size=15), 
        legend.position = "none",
        axis.text.x = element_text(size = 13),
        axis.title.x = element_text(size = 13),
        axis.text.y = element_text(size = 12),
        axis.title.y = element_text(size = 13))
```

Let's compare these estimates with the multi-level Gaussian model:

```{r}
ADS_data <- spread_rvars(Articulation_m3, r_Subject[r_Subject, Intercept], b_Intercept, b_RegisterIDS) %>%
  mutate(Subject_estimate = r_Subject + b_Intercept) %>%
  mutate(Intercept = replace(Intercept, Intercept == "Intercept", "ADS")) %>%
  mutate(Intercept = replace(Intercept, Intercept == "RegisterIDS", "IDS")) %>%
  filter(Intercept == "ADS")

IDS_data <- spread_rvars(Articulation_m3, r_Subject[r_Subject, Intercept], b_Intercept, b_RegisterIDS) %>%
  mutate(Subject_estimate = r_Subject + b_RegisterIDS) %>%
  mutate(Intercept = replace(Intercept, Intercept == "Intercept", "ADS")) %>%
  mutate(Intercept = replace(Intercept, Intercept == "RegisterIDS", "IDS")) %>%
  filter(Intercept == "IDS")

posterior_data_plot <- rbind(ADS_data, IDS_data) %>%
  median_qi(Subject_estimate) %>%
  mutate(Subject = rep(unique(d$Subject), 2))

gaussian_model_plot <- ggplot() +
  geom_point(aes(x = Intercept, y = Subject_estimate), data = posterior_data_plot, size = 2.5, color = "black", ) +
  geom_point(aes(x = Intercept, y = Subject_estimate, color = Subject), data = posterior_data_plot, size = 1.5) +
  geom_path(aes(x = Intercept, y = Subject_estimate, color = Subject, group = Subject), data = posterior_data_plot, alpha = 0.7, linetype = 1) +
  theme_bw() +
  ylim(c(-0.75, 0.3)) +
  xlab('Speech Style') + 
  ylab('Effect Size') +
  ggtitle('Gaussian Model') +
  scale_color_manual(values=viridis(n = 27)) + 
  theme(plot.title = element_text(hjust = 0.5, size=15), 
        legend.position = "none",
        axis.text.x = element_text(size = 13),
        axis.title.x = element_text(size = 13),
        axis.text.y = element_text(size = 12),
        axis.title.y = element_text(size = 13))

comparison_plot <- plot_grid(gaussian_model_plot, student_model_plot, nrow = 1)

comparison_plot
```

Q7: How do the individual trajectories differ between the two models? Why do you think we see these differences? 


__________________________________________________________________________________________________________________________________________



Let's also make an ellipsis plot for the student model:

```{r, warning=FALSE}
### Now the ellipsis plot
## Partial pooling
df_partial <- tibble(
  Subject = rownames(coef(Articulation_student_m3)[["Subject"]][,,"Intercept"]),
  ADS = coef(Articulation_student_m3)[["Subject"]][,,"Intercept"][,1],
  RegisterIDS = coef(Articulation_student_m3)[["Subject"]][,,"RegisterIDS"][,1],
  Type = "Partial pooling"
)
## Original data
df_no <- NULL

for (s in unique(d$Subject)){
  tmp <- tibble(
    Subject = s,
    ADS = d$ArticulationS[d$Register=="ADS" & d$Subject==s],
    RegisterIDS = d$ArticulationS[d$Register=="IDS" & d$Subject==s] - d$ArticulationS[d$Register=="ADS" & d$Subject==s],
    Type = "No pooling"
  )
  if (exists("df_no")){df_no = rbind(df_no, tmp)} else {df_no = tmp}
}
df_total <- df_no[,c("Subject")] %>%
  mutate(
    ADS = mean(d$ArticulationS[d$Register=="ADS"]),
    RegisterIDS =  mean(d$ArticulationS[d$Register=="IDS"]) - mean(d$ArticulationS[d$Register=="ADS"]),
    Type = "Total pooling"
  )
df_fixef <- tibble(
  Type = "Partial pooling (average)",
  ADS = fixef(Articulation_student_m3)[1],
  RegisterIDS = fixef(Articulation_student_m3)[2]
)

# Complete pooling / fixed effects are center of gravity in the plot
df_gravity <- df_total %>% 
  distinct(Type, ADS, RegisterIDS) %>% 
  bind_rows(df_fixef)

df_pulled <- bind_rows(df_no, df_partial)

# Extract the variance covariance matrix
cov_mat_t <- VarCorr(Articulation_student_m3)[["Subject"]]$cov
cov_mat <- matrix(nrow=2, ncol=2)
cov_mat[1,1]<-cov_mat_t[,,"Intercept"][1,1]
cov_mat[2,1]<-cov_mat_t[,,"RegisterIDS"][1,1]
cov_mat[1,2]<-cov_mat_t[,,"Intercept"][2,1]
cov_mat[2,2]<-cov_mat_t[,,"RegisterIDS"][2,1]

make_ellipse <- function(cov_mat, center, level) {
  ellipse(cov_mat, centre = center, level = level) %>%
    as.data.frame() %>%
    add_column(level = level) %>% 
    as_tibble()
}
center <- fixef(Articulation_student_m3)
levels <- c(.1, .3, .5, .7, .9)

# Create an ellipse dataframe for each of the levels defined 
# above and combine them
df_ellipse <- levels %>%
  purrr::map_df(~ make_ellipse(cov_mat, center, level = .x)) %>% 
  dplyr::rename(ADS = x, RegisterIDS = y)

Student_ellipsis <- ggplot(df_pulled) + 
  aes(x = ADS, y = RegisterIDS, color = Type) + 
  # Draw contour lines from the distribution of effects
  geom_path(
    aes(group = level, color = NULL), 
    data = df_ellipse, 
    linetype = "dashed", 
    color = "grey40"
  ) + 
  geom_point(data = df_gravity, size = 5) + 
  geom_point(size = 2) + 
  geom_path(
    aes(group = Subject, color = NULL), 
    arrow = arrow(length = unit(.02, "npc"))
  ) + 
  # Use ggrepel to jitter the labels away from the points
  ggrepel::geom_text_repel(
    aes(label = Subject, color = NULL), 
    data = df_no
  ) + 
  # Don't forget 373
  ggrepel::geom_text_repel(
    aes(label = Subject, color = NULL), 
    data = df_partial
  ) + 
  ggtitle("Topographic map of regression parameters") + 
  xlab("Intercept estimate") + 
  ylab("Slope estimate") + 
  scale_color_brewer(palette = "Dark2") +
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5, size = 15),
        legend.position = "bottom",
        axis.title.x = element_text(size = 13),
        axis.text.y = element_text(size = 12),
        axis.text.x = element_text(size = 12),
        axis.title.y = element_text(size = 13),
        strip.background = element_rect(color="white", fill="white", size=1.5, linetype="solid"))
Student_ellipsis
```

<!--chapter:end:03-part2.Rmd-->

# Informed priors

## Video on Informed priors:

https://youtu.be/zBEiugiqbd4

After watching the videos we recommend you download the Markdown file and go through it in Rstudio:

* https://github.com/4CCoxAU/PriorsWorkshop/blob/08ea1968db8114690b04e839502ea5909300a62f/04-part3.Rmd

Here the data (same data as the previous markdown)

* https://github.com/4CCoxAU/PriorsWorkshop/blob/08ea1968db8114690b04e839502ea5909300a62f/vowel_space_area_data.csv

The content of the markdown is reproduced below.

## Hands-on Exercises

### How to Encode Information in Priors
In this part, we'll have a closer look at how to encode information from previous studies within the prior specifications of our models. Let's start with the meta-analytic prior (i.e., Mean ES: 0.62, SE = 0.13) and add it to our prior specifications for the multi-level model. We do this by specifying the prior for the slope as normal(0.62, 0.13), as in the code block below:

```{r, results="hide", message=FALSE, warning=FALSE}
Articulation_f3 <- bf(ArticulationS ~ 1 + Register + (1+Register|Subject))
meta_analytic_prior <- c(
  prior(normal(0, 1), class = Intercept),
  prior(normal(0, 1), class = sd, coef = Intercept, group = Subject),
  prior(normal(0.62, 0.13), class = b),
  prior(normal(0, 1), class = sd, coef = RegisterIDS, group = Subject),
  prior(normal(1, 0.5), class = sigma),
  prior(lkj(2), class = cor))
Articulation_MAprior_m3 <- 
  brm(
    Articulation_f3,
    data = d,
    save_pars = save_pars(all = TRUE),
    family = gaussian,
    prior = meta_analytic_prior,
    file = "Articulation_MAprior_m3",
    refit = "on_change",
    sample_prior = T,
    iter = 10000, 
    warmup = 1000,
    cores = 2,
    chains = 2,
    backend = "cmdstanr",
    threads = threading(2),
    control = list(
      adapt_delta = 0.999,
      max_treedepth = 20))
```

```{r, warning=FALSE}
pp_check(Articulation_MAprior_m3, ndraws = 50)
plot(conditional_effects(Articulation_MAprior_m3), points = T)
summary(Articulation_MAprior_m3)
```

Q8: How does the slope estimate for this model with the meta-analytic prior compare to that with the skeptical prior (i.e., Articulation_m3)?

__________________________________________________________________________________________________________________________________________


Let's try to do the same analysis for the Danish prior (i.e., Mean ES: -0.1, SE = 0.04):

```{r, results="hide", message=FALSE}
danish_prior <- c(
  prior(normal(0, 1), class = Intercept),
  prior(normal(0, 1), class = sd, coef = Intercept, group = Subject),
  prior(normal(-0.1, 0.04), class = b),
  prior(normal(0, 1), class = sd, coef = RegisterIDS, group = Subject),
  prior(normal(1, 0.5), class = sigma),
  prior(lkj(2), class = cor))
Articulation_Danishprior_m3 <- 
  brm(
    Articulation_f3,
    data = d,
    save_pars = save_pars(all = TRUE),
    family = gaussian,
    prior = danish_prior,
    file = "Articulation_Danishprior_m3",
    refit = "on_change",
    sample_prior = T,
    iter = 10000, 
    warmup = 1000,
    cores = 2,
    chains = 2,
    backend = "cmdstanr",
    threads = threading(2),
    control = list(
      adapt_delta = 0.999,
      max_treedepth = 20))
```

```{r}
pp_check(Articulation_Danishprior_m3, ndraws = 50)
plot(conditional_effects(Articulation_Danishprior_m3), points = T)
summary(Articulation_Danishprior_m3)
```

Q9: How does the slope estimate for this model with the Danish prior compare to that with the skeptical prior (i.e., Articulation_m3)?

__________________________________________________________________________________________________________________________________________

### Prior-Posterior Updates Plot

Let's run the following code to extract and visualise the prior and posterior distributions from the different models:

```{r, warning=FALSE}
danish_prior_posterior <- as_draws_df(Articulation_Danishprior_m3) %>%
  mutate(priors = "Danish") %>%
  select(prior_b, b_RegisterIDS, priors)
  
ma_prior_posterior <- as_draws_df(Articulation_MAprior_m3) %>%
  mutate(priors = "MA estimates") %>%
  select(prior_b, b_RegisterIDS, priors)
  
skeptical_prior_posterior <- as_draws_df(Articulation_m3) %>%
  mutate(priors = "Skeptical estimates") %>%
  select(prior_b, b_RegisterIDS, priors)
Posterior <- rbind(danish_prior_posterior, ma_prior_posterior, skeptical_prior_posterior)
plot1 <- ggplot(Posterior) + 
  theme_classic() +
  ggtitle("Priors") +
  geom_density(aes(x = prior_b, fill = priors), alpha = 0.7) +
  xlim(c(-1.5, 1.5)) +
  geom_vline(xintercept = 0.0, linetype = 3) +
  scale_fill_manual(name = "Prior",
                  labels = c('Meta-analytic', "Danish", "Skeptical"),
                  values=c("#FC4E07", "steelblue", "#228B22")) +
  theme(plot.title = element_text(hjust = 0.5, size=15),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        axis.title.y=element_blank(),
        axis.line.x = element_line(size=0.1),
        axis.line.y = element_line(size=0.0),
        axis.text.x = element_blank(),
        axis.title.x=element_blank(),
        legend.position = "none")
plot2 <- ggplot(Posterior) + 
  theme_classic() +
  xlim(c(-1.5, 1.5)) +
  geom_density(aes(x = b_RegisterIDS, fill = priors), alpha = 0.7) +
  xlab('Effect Size') +
  ggtitle(expression(paste("Posteriors"))) +
  geom_vline(xintercept = 0.0, linetype = 3) +
  scale_fill_manual(name = "Priors:",
                  labels = c('Danish', 'Meta-analytic', "Skeptical"),
                  values=c("#FC4E07", "steelblue", "#228B22")) +
  scale_color_manual(values=c("#FC4E07","#228B22", "steelblue")) + 
  theme(plot.title = element_text(hjust = 0.5, size=15),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        axis.title.y=element_blank(),
        axis.line = element_line(size=0),
        legend.position = "bottom")
priors_posteriors_plot <- plot_grid(plot1, plot2, ncol=1)
priors_posteriors_plot
```


Q10: Why does the model with the Danish study prior not shift as much as those with the skeptical and meta-analytic priors?

Answer:


__________________________________________________________________________________________________________________________________________


Q11: What does this tell us about the nature of evidence accumulation in Bayesian models?

Answer:


__________________________________________________________________________________________________________________________________________

<!--chapter:end:04-part3.Rmd-->

# Recommendations

## Where to go for further study
1. We would highly recommend McElreath’s Statistical Rethinking book and online video lectures: (https://www.youtube.com/channel/UCNJK6_DZvcMqNSzQdEkzvzA)
2. Michael Betancourt has awesome materials (once you already master McElreath’s level) (https://betanalpha.github.io/writing/)
3. Potsdam Summer School on Statistical Methods for Linguistics and Psychology - with a line on Bayesian Statistics (https://vasishth.github.io/smlp2021/)
4. Nicenboim, Schad & Vasisht - An Introduction to Bayesian Data Analysis for Cognitive Science: https://vasishth.github.io/Bayes_CogSci/

## Feedback on the course

Please help us improve this course by answering the following form:
https://forms.gle/LDsmPKYM33udV5Qg6

<!--chapter:end:05-part4.Rmd-->

